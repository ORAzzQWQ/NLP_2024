{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"046d7e1f-b9a1-46a7-ab0e-3002d9b430f6","_uuid":"31f6f238-8d0a-42df-ac38-aa8b42e44caa","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"source":["## Import"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c7cede4a-2e8f-4690-b49e-f5eb5712ab22","_uuid":"3e1fd6f5-8b04-4330-b94b-880c61d25db9","collapsed":false,"execution":{"iopub.execute_input":"2024-12-26T12:18:52.269674Z","iopub.status.busy":"2024-12-26T12:18:52.269209Z","iopub.status.idle":"2024-12-26T12:18:52.275963Z","shell.execute_reply":"2024-12-26T12:18:52.274942Z","shell.execute_reply.started":"2024-12-26T12:18:52.269645Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import re\n","import copy\n","import numpy as np\n","import pandas as pd\n","import polars as pl\n","import lightgbm as lgb\n","from tqdm.auto import tqdm,trange\n","from lightgbm import log_evaluation, early_stopping\n","from sklearn.model_selection import KFold,StratifiedKFold\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics import cohen_kappa_score, accuracy_score\n","import joblib  # 新增 joblib 用於載入 DeBERTa 預測結果\n","import gc\n","import torch\n","from glob import glob\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n","from datasets import Dataset\n","from scipy.special import softmax\n","from datasets import Dataset"]},{"cell_type":"markdown","metadata":{"_cell_guid":"eaab5984-ac7a-435c-9523-d962d72bb6e9","_uuid":"9ba0ebb0-d6eb-40a7-86aa-9f69e277661b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"source":["## Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"02853f3d-b2ee-4a49-b43a-45e176aeee25","_uuid":"8348d1c3-818a-4372-bf5d-d0db532c000b","collapsed":false,"execution":{"iopub.execute_input":"2024-12-26T12:18:52.278098Z","iopub.status.busy":"2024-12-26T12:18:52.277838Z","iopub.status.idle":"2024-12-26T12:18:52.483991Z","shell.execute_reply":"2024-12-26T12:18:52.483112Z","shell.execute_reply.started":"2024-12-26T12:18:52.278072Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Load training and testing sets, while using \\n \\n character segmentation to list and renaming to paragraph for full_text data\n","columns = [  \n","    (\n","        pl.col(\"full_text\").str.split(by=\"\\n\\n\").alias(\"paragraph\")\n","    ),\n","]\n","PATH = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/\"\n","\n","train = pl.read_csv(PATH + \"train.csv\").with_columns(columns)\n","test = pl.read_csv(PATH + \"test.csv\").with_columns(columns)\n","\n","# Display the first sample data in the training set\n","train.head(1)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"ead5a3de-ed90-4112-8b7a-c28a86a4e791","_uuid":"e1e2f803-8230-4d1e-bf19-6308ebddf26d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"source":["## Features engineering"]},{"cell_type":"markdown","metadata":{"_cell_guid":"a0e3dc12-6f31-49c1-86a0-c50bdfb3cf0d","_uuid":"d23c0a5f-ea2e-4973-95ec-9e060dc4d231","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"source":["### 1.Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4d6ae2da-dcdc-4d1c-9e93-b35b61b8f4f2","_uuid":"d5163cfd-73b8-46b2-a133-ce4ded2f9d2c","collapsed":false,"execution":{"iopub.execute_input":"2024-12-26T12:18:52.485332Z","iopub.status.busy":"2024-12-26T12:18:52.485069Z","iopub.status.idle":"2024-12-26T12:18:52.491278Z","shell.execute_reply":"2024-12-26T12:18:52.490361Z","shell.execute_reply.started":"2024-12-26T12:18:52.485312Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def removeHTML(x):\n","    html=re.compile(r'<.*?>')\n","    return html.sub(r'',x)\n","def dataPreprocessing(x):\n","    # Convert words to lowercase\n","    x = x.lower()\n","    # Remove HTML\n","    x = removeHTML(x)\n","    # Delete strings starting with @\n","    x = re.sub(\"@\\w+\", '',x)\n","    # Delete Numbers\n","    x = re.sub(\"'\\d+\", '',x)\n","    x = re.sub(\"\\d+\", '',x)\n","    # Delete URL\n","    x = re.sub(\"http\\w+\", '',x)\n","    # Replace consecutive empty spaces with a single space character\n","    x = re.sub(r\"\\s+\", \" \", x)\n","    # Replace consecutive commas and periods with one comma and period character\n","    x = re.sub(r\"\\.+\", \".\", x)\n","    x = re.sub(r\"\\,+\", \",\", x)\n","    # Remove empty characters at the beginning and end\n","    x = x.strip()\n","    return x"]},{"cell_type":"markdown","metadata":{"_cell_guid":"df13dc69-eda1-4eb7-af14-061fb77d4410","_uuid":"4270c9e6-9dae-4318-97f1-66f0ac81b690","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"source":["### 2.Paragraph Features"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"36053483-d14b-450f-b77a-2baf7f252114","_uuid":"85f330ef-6be6-4c39-9efe-fceb4e0234c2","collapsed":false,"execution":{"iopub.execute_input":"2024-12-26T12:18:52.492789Z","iopub.status.busy":"2024-12-26T12:18:52.492471Z","iopub.status.idle":"2024-12-26T12:18:57.635905Z","shell.execute_reply":"2024-12-26T12:18:57.634930Z","shell.execute_reply.started":"2024-12-26T12:18:52.492768Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# paragraph features\n","def Paragraph_Preprocess(tmp):\n","    # Expand the paragraph list into several lines of data\n","    tmp = tmp.explode('paragraph')\n","    # Paragraph preprocessing\n","    tmp = tmp.with_columns(pl.col('paragraph').map_elements(dataPreprocessing))\n","    # Calculate the length of each paragraph\n","    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x)).alias(\"paragraph_len\"))\n","    # Calculate the number of sentences and words in each paragraph\n","    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x.split('.'))).alias(\"paragraph_sentence_cnt\"),\n","                    pl.col('paragraph').map_elements(lambda x: len(x.split(' '))).alias(\"paragraph_word_cnt\"),)\n","    return tmp\n","\n","# feature_eng\n","paragraph_fea = ['paragraph_len','paragraph_sentence_cnt','paragraph_word_cnt']\n","def Paragraph_Eng(train_tmp):\n","    aggs = [\n","        # Count the number of paragraph lengths greater than and less than the i-value\n","        *[pl.col('paragraph').filter(pl.col('paragraph_len') >= i).count().alias(f\"paragraph_{i}_cnt\") for i in [50,75,100,125,150,175,200,250,300,350,400,500,600,700] ], \n","        *[pl.col('paragraph').filter(pl.col('paragraph_len') <= i).count().alias(f\"paragraph_{i}_cnt\") for i in [25,49]], \n","        # other\n","        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in paragraph_fea],\n","        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in paragraph_fea],\n","        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in paragraph_fea],\n","        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in paragraph_fea],\n","        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in paragraph_fea],\n","        ]\n","    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n","    df = df.to_pandas()\n","    return df\n","\n","tmp = Paragraph_Preprocess(train)\n","train_feats = Paragraph_Eng(tmp)\n","train_feats['score'] = train['score']\n","\n","# Obtain feature names\n","feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n","print('Features Number: ',len(feature_names))\n","train_feats.head(3)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"ddd84073-eee0-4940-8bc4-65e37d94b280","_uuid":"6e871e76-bde2-4776-b3dd-abc8bf2a9fe4","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"source":["### 3.Sentence Features"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eb2aad08-2e86-427a-a738-3ac3f55cfa81","_uuid":"39d15e72-bd32-4081-ab91-3e46210cbe7a","collapsed":false,"execution":{"iopub.execute_input":"2024-12-26T12:18:57.638382Z","iopub.status.busy":"2024-12-26T12:18:57.638103Z","iopub.status.idle":"2024-12-26T12:19:02.790126Z","shell.execute_reply":"2024-12-26T12:19:02.789232Z","shell.execute_reply.started":"2024-12-26T12:18:57.638360Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# sentence feature\n","def Sentence_Preprocess(tmp):\n","    # Preprocess full_text and use periods to segment sentences in the text\n","    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\".\").alias(\"sentence\"))\n","    tmp = tmp.explode('sentence')\n","    # Calculate the length of a sentence\n","    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x)).alias(\"sentence_len\"))\n","    # Filter out the portion of data with a sentence length greater than 15\n","    tmp = tmp.filter(pl.col('sentence_len')>=15)\n","    # Count the number of words in each sentence\n","    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x.split(' '))).alias(\"sentence_word_cnt\"))\n","    \n","    return tmp\n","\n","# feature_eng\n","sentence_fea = ['sentence_len','sentence_word_cnt']\n","def Sentence_Eng(train_tmp):\n","    aggs = [\n","        # Count the number of sentences with a length greater than i\n","        *[pl.col('sentence').filter(pl.col('sentence_len') >= i).count().alias(f\"sentence_{i}_cnt\") for i in [15,50,100,150,200,250,300] ], \n","        # other\n","        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in sentence_fea],\n","        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in sentence_fea],\n","        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in sentence_fea],\n","        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in sentence_fea],\n","        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in sentence_fea],\n","        ]\n","    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n","    df = df.to_pandas()\n","    return df\n","\n","tmp = Sentence_Preprocess(train)\n","# Merge the newly generated feature data with the previously generated feature data\n","train_feats = train_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n","\n","feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n","print('Features Number: ',len(feature_names))\n","train_feats.head(3)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"974ac041-96b0-449b-ade1-a800e3f45450","_uuid":"01b20980-3d77-49df-99fe-6bdf702d9279","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"source":["### 4.Word Features"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"22a639be-0798-4242-b123-dd73096cb6f2","_uuid":"56647ea8-a28f-4990-bd02-0e78be3dc666","collapsed":false,"execution":{"iopub.execute_input":"2024-12-26T12:19:02.792145Z","iopub.status.busy":"2024-12-26T12:19:02.791628Z","iopub.status.idle":"2024-12-26T12:19:13.484329Z","shell.execute_reply":"2024-12-26T12:19:13.483455Z","shell.execute_reply.started":"2024-12-26T12:19:02.792110Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# word feature\n","def Word_Preprocess(tmp):\n","    # Preprocess full_text and use spaces to separate words from the text\n","    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\" \").alias(\"word\"))\n","    tmp = tmp.explode('word')\n","    # Calculate the length of each word\n","    tmp = tmp.with_columns(pl.col('word').map_elements(lambda x: len(x)).alias(\"word_len\"))\n","    # Delete data with a word length of 0\n","    tmp = tmp.filter(pl.col('word_len')!=0)\n","    return tmp\n","\n","# feature_eng\n","def Word_Eng(train_tmp):\n","    aggs = [\n","        # Count the number of words with a length greater than i+1\n","        *[pl.col('word').filter(pl.col('word_len') >= i+1).count().alias(f\"word_{i+1}_cnt\") for i in range(15) ], \n","        # other\n","        pl.col('word_len').max().alias(f\"word_len_max\"),\n","        pl.col('word_len').mean().alias(f\"word_len_mean\"),\n","        pl.col('word_len').std().alias(f\"word_len_std\"),\n","        pl.col('word_len').quantile(0.25).alias(f\"word_len_q1\"),\n","        pl.col('word_len').quantile(0.50).alias(f\"word_len_q2\"),\n","        pl.col('word_len').quantile(0.75).alias(f\"word_len_q3\"),\n","        ]\n","    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n","    df = df.to_pandas()\n","    return df\n","\n","tmp = Word_Preprocess(train)\n","# Merge the newly generated feature data with the previously generated feature data\n","train_feats = train_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n","\n","feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n","print('Features Number: ',len(feature_names))\n","train_feats.head(3)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"8eb342df-2d2b-4da7-9b5f-667729e21af7","_uuid":"ce39af92-b464-48c2-8faa-6e6699b8e9f1","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"source":["### 5.Tf-idf features"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9f5adb85-4024-42f2-b7de-91ba25388b32","_uuid":"78d46ac9-7309-42ed-9c52-c491d916679a","collapsed":false,"execution":{"iopub.execute_input":"2024-12-26T12:19:13.485676Z","iopub.status.busy":"2024-12-26T12:19:13.485383Z","iopub.status.idle":"2024-12-26T12:20:00.954494Z","shell.execute_reply":"2024-12-26T12:20:00.953571Z","shell.execute_reply.started":"2024-12-26T12:19:13.485653Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# TfidfVectorizer parameter\n","vectorizer = TfidfVectorizer(\n","            tokenizer=lambda x: x,\n","            preprocessor=lambda x: x,\n","            token_pattern=None,\n","            strip_accents='unicode',\n","            analyzer = 'word',\n","            ngram_range=(1,3),\n","            min_df=0.05,\n","            max_df=0.95,\n","            sublinear_tf=True,\n",")\n","# Fit all datasets into TfidfVector,this may cause leakage and overly optimistic CV scores\n","train_tfid = vectorizer.fit_transform([i for i in train['full_text']])\n","# Convert to array\n","dense_matrix = train_tfid.toarray()\n","# Convert to dataframe\n","df = pd.DataFrame(dense_matrix)\n","# rename features\n","tfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\n","df.columns = tfid_columns\n","df['essay_id'] = train_feats['essay_id']\n","# Merge the newly generated feature data with the previously generated feature data\n","train_feats = train_feats.merge(df, on='essay_id', how='left')\n","\n","feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n","print('Features Number: ',len(feature_names))\n","train_feats.head(3)"]},{"cell_type":"markdown","metadata":{},"source":["### 6. DeBERTa features\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-26T12:20:00.955773Z","iopub.status.busy":"2024-12-26T12:20:00.955494Z","iopub.status.idle":"2024-12-26T12:20:00.966644Z","shell.execute_reply":"2024-12-26T12:20:00.965813Z","shell.execute_reply.started":"2024-12-26T12:20:00.955751Z"},"trusted":true},"outputs":[],"source":["# Add DeBERTa predictions as features\n","# Use pre-trained DeBERTa model on training set\n","deberta_oof = joblib.load('/kaggle/input/aes2-400-20240419134941/oof.pkl')\n","for i in range(6):\n","    train_feats[f'deberta_oof_{i}'] = deberta_oof[:, i]\n","\n","feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n","print('Features number: ',len(feature_names))\n","train_feats.head(3)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"028f31e8-4020-4c7e-b6f7-4acdddcb0b9e","_uuid":"9e51c3b4-49ee-41eb-8c4f-4eabc8df955a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"source":["## Train\n","* I have trained and saved the model\n","* you can choose to retrain or load the model"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bb3fa6a8-cfe1-4d9c-88c9-5b19f794a21d","_uuid":"085b6dc3-a84c-4aff-a88f-10efb250c9fa","collapsed":false,"execution":{"iopub.execute_input":"2024-12-26T12:20:00.974791Z","iopub.status.busy":"2024-12-26T12:20:00.974553Z","iopub.status.idle":"2024-12-26T12:20:00.985411Z","shell.execute_reply":"2024-12-26T12:20:00.984557Z","shell.execute_reply.started":"2024-12-26T12:20:00.974772Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# idea from https://www.kaggle.com/code/rsakata/optimize-qwk-by-lgb/notebook#QWK-objective\n","\n","def quadratic_weighted_kappa(y_true, y_pred):\n","    y_true = y_true + a\n","    y_pred = (y_pred + a).clip(1, 6).round()\n","    qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n","    return 'QWK', qwk, True\n","    \n","def qwk_obj(y_true, y_pred):\n","    labels = y_true + a\n","    preds = y_pred + a\n","    preds = preds.clip(1, 6)\n","    f = 1/2*np.sum((preds-labels)**2)\n","    g = 1/2*np.sum((preds-a)**2+b)\n","    df = preds - labels\n","    dg = preds - a\n","    grad = (df/g - f*dg/g**2)*len(labels)\n","    hess = np.ones(len(labels))\n","    return grad, hess\n","\n","a = 2.948\n","b = 1.092"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1d3a0870-1716-4e08-af32-b1fcc9c90532","_uuid":"60e3d0c3-8230-476a-93a2-bde0b4a89b22","collapsed":false,"execution":{"iopub.execute_input":"2024-12-26T12:20:00.987072Z","iopub.status.busy":"2024-12-26T12:20:00.986829Z","iopub.status.idle":"2024-12-26T12:22:33.775687Z","shell.execute_reply":"2024-12-26T12:22:33.774861Z","shell.execute_reply.started":"2024-12-26T12:20:00.987054Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["LOAD = False\n","models = []\n","if LOAD:\n","    for i in range(5):\n","        models.append(lgb.Booster(model_file=f'../input/lal-lgb-baseline-4/fold_{i}.txt'))\n","else:\n","    # out-of-fold (OOF) is used to store the prediction results of each model on the validation set\n","    oof = []\n","    x= train_feats\n","    y= train_feats['score'].values\n","\n","    # 5 fold\n","    kfold = KFold(n_splits=5, random_state=42, shuffle=True)\n","    callbacks = [log_evaluation(period=25), early_stopping(stopping_rounds=75,first_metric_only=True)]\n","    for fold_id, (trn_idx, val_idx) in tqdm(enumerate(kfold.split(x.copy(), y.copy().astype(str)))):\n","            # create model\n","            model = lgb.LGBMRegressor(\n","                objective = qwk_obj,\n","                metrics = 'None',\n","                learning_rate = 0.1,\n","                max_depth = 5,\n","                num_leaves = 10,\n","                colsample_bytree=0.5,\n","                reg_alpha = 0.1,\n","                reg_lambda = 0.8,\n","                n_estimators=1024,\n","                random_state=42,\n","                verbosity = - 1)\n","            # Take out the training and validation sets for 5 kfold segmentation separately\n","            X_train = train_feats.iloc[trn_idx][feature_names]\n","            Y_train = train_feats.iloc[trn_idx]['score'] - a\n","\n","            X_val = train_feats.iloc[val_idx][feature_names]\n","            Y_val = train_feats.iloc[val_idx]['score'] - a\n","            print('\\nFold_{} Training ================================\\n'.format(fold_id+1))\n","  \n","            # Training model\n","            lgb_model = model.fit(X_train,\n","                                  Y_train,\n","                                  eval_names=['train', 'valid'],\n","                                  eval_set=[(X_train, Y_train), (X_val, Y_val)],\n","                                  eval_metric=quadratic_weighted_kappa,\n","                                  callbacks=callbacks,)\n","\n","            # Use the trained model to predict the validation set\n","            pred_val = lgb_model.predict(\n","                X_val, num_iteration=lgb_model.best_iteration_)\n","            df_tmp = train_feats.iloc[val_idx][['essay_id', 'score']].copy()\n","            df_tmp['pred'] = pred_val + a\n","            oof.append(df_tmp)\n","\n","            # Save model parameters\n","            models.append(model.booster_)\n","            lgb_model.booster_.save_model(f'/kaggle/working/fold_{fold_id}.txt')\n","    df_oof = pd.concat(oof)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"670e9923-6e46-440d-824a-c372d45eb112","_uuid":"7be36b00-c99f-4fac-9b40-8c4150969098","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"source":["### CV"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bb8807a7-c9ed-40b0-8ad5-07ef34cf816d","_uuid":"5c6d5868-ec25-46e4-b7ea-1423565289f9","collapsed":false,"execution":{"iopub.execute_input":"2024-12-26T12:22:33.777186Z","iopub.status.busy":"2024-12-26T12:22:33.776861Z","iopub.status.idle":"2024-12-26T12:22:33.803357Z","shell.execute_reply":"2024-12-26T12:22:33.802653Z","shell.execute_reply.started":"2024-12-26T12:22:33.777163Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["acc = accuracy_score(df_oof['score'], df_oof['pred'].clip(1, 6).round())\n","kappa = cohen_kappa_score(df_oof['score'], df_oof['pred'].clip(1, 6).round(), weights=\"quadratic\")\n","print('acc: ',acc)\n","print('kappa: ',kappa)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"274e6fb4-edfa-4b1e-b750-6a10dc7b6af9","_uuid":"1c5b971d-64e8-44f3-80af-824983fe6d97","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"source":["## Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-26T12:22:33.805746Z","iopub.status.busy":"2024-12-26T12:22:33.805470Z","iopub.status.idle":"2024-12-26T12:22:48.195916Z","shell.execute_reply":"2024-12-26T12:22:48.194905Z","shell.execute_reply.started":"2024-12-26T12:22:33.805726Z"},"trusted":true},"outputs":[],"source":["MAX_LENGTH = 1024\n","MODEL_PATH = '/kaggle/input/aes2-400-20240419134941/*/*'\n","EVAL_BATCH_SIZE = 1\n","\n","# Get DeBERTa models\n","deberta_models = glob(MODEL_PATH)\n","print(f\"Found {len(deberta_models)} models\")\n","\n","# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(deberta_models[0])\n","\n","def tokenize(sample):\n","    \"\"\"Tokenize the text using the pre-trained tokenizer.\"\"\"\n","    return tokenizer(sample['full_text'], max_length=MAX_LENGTH, truncation=True)\n","\n","# Prepare test dataset\n","df_test = pd.read_csv(PATH + \"test.csv\")\n","ds = Dataset.from_pandas(df_test).map(tokenize).remove_columns(['essay_id', 'full_text'])\n","\n","# Setup evaluation arguments\n","args = TrainingArguments(\n","    output_dir=\".\", \n","    per_device_eval_batch_size=EVAL_BATCH_SIZE, \n","    report_to=\"none\"\n",")\n","\n","# Get predictions from all models\n","predictions = []\n","for model_path in deberta_models:\n","    # Use pre-trained DeBERTa model on training set\n","    print(f\"Processing model: {model_path}\")\n","    \n","    # Load model\n","    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n","    \n","    # Setup trainer\n","    trainer = Trainer(\n","        model=model, \n","        args=args, \n","        data_collator=DataCollatorWithPadding(tokenizer), \n","        tokenizer=tokenizer\n","    )\n","    \n","    # Get predictions\n","    preds = trainer.predict(ds).predictions\n","    predictions.append(softmax(preds, axis=-1))\n","    \n","    # Cleanup\n","    del model, trainer\n","    torch.cuda.empty_cache()\n","    gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-26T12:22:48.198169Z","iopub.status.busy":"2024-12-26T12:22:48.197466Z","iopub.status.idle":"2024-12-26T12:22:48.202746Z","shell.execute_reply":"2024-12-26T12:22:48.201847Z","shell.execute_reply.started":"2024-12-26T12:22:48.198135Z"},"trusted":true},"outputs":[],"source":["# Aggregate predictions from multiple models by averaging\n","predicted_score = 0.0\n","for p in predictions:\n","    predicted_score += p\n","\n","# Calculate the average predicted score\n","predicted_score /= len(predictions)"]},{"cell_type":"markdown","metadata":{},"source":["## Submission"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"651c289f-efec-492a-9a13-885afbe4f58b","_uuid":"85596c6a-4ed1-4e7c-b841-86bab3bdc881","collapsed":false,"execution":{"iopub.execute_input":"2024-12-26T12:22:48.204497Z","iopub.status.busy":"2024-12-26T12:22:48.203867Z","iopub.status.idle":"2024-12-26T12:22:48.575077Z","shell.execute_reply":"2024-12-26T12:22:48.574210Z","shell.execute_reply.started":"2024-12-26T12:22:48.204462Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Paragraph\n","tmp = Paragraph_Preprocess(test)\n","test_feats = Paragraph_Eng(tmp)\n","# Sentence\n","tmp = Sentence_Preprocess(test)\n","test_feats = test_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n","# Word\n","tmp = Word_Preprocess(test)\n","test_feats = test_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n","# Tfidf\n","test_tfid = vectorizer.transform([i for i in test['full_text']])\n","dense_matrix = test_tfid.toarray()\n","df = pd.DataFrame(dense_matrix)\n","tfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\n","df.columns = tfid_columns\n","df['essay_id'] = test_feats['essay_id']\n","test_feats = test_feats.merge(df, on='essay_id', how='left')\n","# deberta\n","for i in range(6):\n","    test_feats[f'deberta_oof_{i}'] = predicted_score[:, i]\n","\n","# Features number\n","feature_names = list(filter(lambda x: x not in ['essay_id','score'], test_feats.columns))\n","print('Features number: ',len(feature_names))\n","test_feats.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"09f96a70-8024-411d-8aef-9193d5f9116b","_uuid":"7b19093f-e89c-40ba-9449-63fb8357c6ea","collapsed":false,"execution":{"iopub.execute_input":"2024-12-26T12:22:48.576565Z","iopub.status.busy":"2024-12-26T12:22:48.576249Z","iopub.status.idle":"2024-12-26T12:22:48.629137Z","shell.execute_reply":"2024-12-26T12:22:48.627923Z","shell.execute_reply.started":"2024-12-26T12:22:48.576507Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["prediction = test_feats[['essay_id']].copy()\n","prediction['score'] = 0\n","pred_test = models[0].predict(test_feats[feature_names]) + a\n","for i in range(4):\n","    pred_now = models[i+1].predict(test_feats[feature_names]) + a\n","    pred_test = np.add(pred_test,pred_now)\n","\n","# The final prediction result needs to be divided by 5 because the prediction results of 5 models were added together\n","pred_test = pred_test / 5\n","print(pred_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4dddab7c-fd6a-4f15-a334-8a6c47b987c1","_uuid":"0ea53fde-81cf-4de7-a535-5642e6fe24f0","collapsed":false,"execution":{"iopub.execute_input":"2024-12-26T12:22:48.630595Z","iopub.status.busy":"2024-12-26T12:22:48.630281Z","iopub.status.idle":"2024-12-26T12:22:48.643796Z","shell.execute_reply":"2024-12-26T12:22:48.642809Z","shell.execute_reply.started":"2024-12-26T12:22:48.630570Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Round the prediction result to an integer and limit it to a range of 1-6 (score range)\n","pred_test = pred_test.clip(1, 6).round()\n","prediction['score'] = pred_test\n","prediction.to_csv('submission.csv', index=False)\n","prediction.head(3)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"474d7d1e-8655-48de-9be2-f0aafc6d8e16","_uuid":"7ae449c0-a6cb-4167-95d8-e4856ad9de0c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"source":["## Reference Notebook\n","- https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-cv-0-799-lb-0-799 (Main)\n","- https://www.kaggle.com/code/davidjlochner/base-tfidf-lgbm\n","- https://www.kaggle.com/code/yunsuxiaozi/aes2-0-baseline-naivebayesclassifier\n","- https://www.kaggle.com/code/finlay/llm-detect-0-to-1\n","- https://www.kaggle.com/code/awqatak/silver-bullet-single-model-165-features\n","- https://www.kaggle.com/code/hiarsl/feature-engineering-sentence-paragraph-features\n","- https://www.kaggle.com/code/rsakata/optimize-qwk-by-lgb/notebook#QWK-objective\n","- https://www.kaggle.com/code/ryenhails/deberta-lgbm-with-detailed-code-comments"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8059942,"sourceId":71485,"sourceType":"competition"},{"datasetId":4762179,"sourceId":8070853,"sourceType":"datasetVersion"},{"datasetId":4767886,"sourceId":8078642,"sourceType":"datasetVersion"},{"datasetId":4832208,"sourceId":8859067,"sourceType":"datasetVersion"}],"dockerImageVersionId":30673,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
