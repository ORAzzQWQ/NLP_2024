# -*- coding: utf-8 -*-
"""HW3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MVDP78NTAoxPWslh5IuasbCuyIB7Sf6L
"""

# !pip install torch==2.4.0
# !pip install transformers==4.37.0
!pip install datasets==2.21.0
!pip install accelerate==0.21.0
!pip install scikit-learn==1.5.2
!pip install torchmetrics

import transformers as T
from datasets import load_dataset
import torch
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from tqdm import tqdm
from torchmetrics import SpearmanCorrCoef, Accuracy, F1Score
import matplotlib.pyplot as plt
device = "cuda" if torch.cuda.is_available() else "cpu"

# 有些中文的標點符號在tokenizer編碼以後會變成[UNK]，所以將其換成英文標點
token_replacement = [
    ["：" , ":"],
    ["，" , ","],
    ["“" , "\""],
    ["”" , "\""],
    ["？" , "?"],
    ["……" , "..."],
    ["！" , "!"]
]

class SemevalDataset(Dataset):
    def __init__(self, split="train") -> None:
        super().__init__()
        assert split in ["train", "validation"]
        self.data = load_dataset(
            "sem_eval_2014_task_1", split=split, cache_dir="./cache/"
        ).to_list()

    def __getitem__(self, index):
        d = self.data[index]
        # 把中文標點替換掉
        for k in ["premise", "hypothesis"]:
            for tok in token_replacement:
                d[k] = d[k].replace(tok[0], tok[1])
        return d

    def __len__(self):
        return len(self.data)

data_sample = SemevalDataset(split="train").data[:3]
print(f"Dataset example: \n{data_sample[0]} \n{data_sample[1]} \n{data_sample[2]}")

train_dataset = SemevalDataset(split="train")
validation_dataset = SemevalDataset(split="validation")

tokenizer = T.BertTokenizer.from_pretrained("google-bert/bert-base-uncased", cache_dir="./cache/")

# Define the hyperparameters
lr = 3e-5
epochs = 3
train_batch_size = 8
validation_batch_size = 8

# TODO1: Create batched data for DataLoader
# `collate_fn` is a function that defines how the data batch should be packed.
# This function will be called in the DataLoader to pack the data batch.
# ASK WITH Claude

def collate_fn(batch):
    # TODO1-1: Implement the collate_fn function
    # Write your code here
    # The input parameter is a data batch (tuple), and this function packs it into tensors.
    premises = [item['premise'] for item in batch]
    hypotheses = [item['hypothesis'] for item in batch]
    entailment_labels = torch.tensor([item['entailment_judgment'] for item in batch])
    relatedness_score = torch.tensor([item['relatedness_score'] for item in batch])

    # Use tokenizer to pack tokenize and pack the data and its corresponding labels.
    encoded = tokenizer(
        premises,
        hypotheses,
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors='pt'
    )

    # Return the data batch and labels for each sub-task.
    return {
        'input_ids': encoded['input_ids'],
        'attention_mask': encoded['attention_mask'], # tokenizer自動生成的mask [pad]的東東
        'entailment_labels': entailment_labels,
        'relatedness_score': relatedness_score
    }

# TODO1-2: Define your DataLoader
dl_train = DataLoader(
    train_dataset,
    batch_size=train_batch_size,
    shuffle=True,
    collate_fn=collate_fn
)
dl_validation = DataLoader(
    validation_dataset,
    batch_size=validation_batch_size,
    shuffle=False,
    collate_fn=collate_fn
)

# TODO2: Construct your model
# ASK WITH Claude

class MultiLabelModel(torch.nn.Module):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # Write your code here
        # Define what modules you will use in the model
        self.bert = T.BertModel.from_pretrained('bert-base-uncased') # 預設是可以更新的

        # # 可以選擇只更新某些層
        # for param in self.bert.embeddings.parameters():
        #     param.requires_grad = False  # 凍結 embedding 層

        # for i, layer in enumerate(self.bert.encoder.layer):
        #     # 例如：只更新最後4層
        #     if i < 8:  # BERT有12層，凍結前8層
        #         for param in layer.parameters():
        #             param.requires_grad = False

        # 加入處理長句子的pooling層
        self.attention_weights = torch.nn.Linear(768, 1)
        self.dropout = torch.nn.Dropout(0.1)

        # 1. 相似度分數 (回歸任務)
        self.similarity_head = torch.nn.Linear(768, 1)
        # 2. 蘊含判斷 (分類任務)
        self.entailment_head = torch.nn.Linear(768, 3)
        # 3. 新增置信度預測層
        self.confidence_head = torch.nn.Linear(768, 2) # 分別預測相似度和蘊含的置信度

    def forward(self, input_ids, attention_mask): # 明確定義需要的參數,不然會報錯(ASK WITH Claude)
        # Write your code here
        # Forward pass
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        sequence_output = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]

        # 計算attention weights
        attention_weights = self.attention_weights(sequence_output).squeeze(-1)  # [batch_size, seq_len]
        attention_weights = attention_weights.masked_fill(~attention_mask.bool(), float('-inf'))
        attention_weights = torch.softmax(attention_weights, dim=-1)

        # 加權平均得到句子表示
        weighted_output = torch.bmm(attention_weights.unsqueeze(1), sequence_output).squeeze(1)
        pooled_output = self.dropout(weighted_output)

        # 產生預測
        similarity_scores = self.similarity_head(pooled_output).squeeze(-1)
        entailment_logits = self.entailment_head(pooled_output)
        confidence_scores = self.confidence_head(pooled_output) # 新增:使用confidence_head

        return similarity_scores, entailment_logits, confidence_scores

# ASK WITH Claude
# 預測錯誤時的懲罰

class PenalizedLoss(torch.nn.Module):
    def __init__(self,
                 sim_threshold=3.0,
                 overestimation_penalty=1.5,  # 高估懲罰係數
                 confidence_penalty=0.1):
        super().__init__()
        self.sim_threshold = sim_threshold
        self.overestimation_penalty = overestimation_penalty  # 新增
        self.confidence_penalty = confidence_penalty
        self.sim_base_loss = torch.nn.MSELoss()
        self.ent_base_loss = torch.nn.CrossEntropyLoss(
            weight=torch.tensor([1.2, 0.8, 1.0]).to(device)
        )

    def forward(self, sim_preds, sim_labels, ent_preds, ent_labels, confidence_scores):
        # 基礎損失
        sim_loss = self.sim_base_loss(sim_preds, sim_labels)
        ent_loss = self.ent_base_loss(ent_preds, ent_labels)

        # 高估懲罰
        overestimation = torch.clamp(sim_preds - sim_labels, min=0)  # 只懲罰高估部分
        overestimation_loss = torch.mean(overestimation ** 2) * self.overestimation_penalty

        # 原有的相似度懲罰
        high_sim_mask = sim_labels >= self.sim_threshold
        if high_sim_mask.any():
            high_sim_penalty = self.sim_base_loss(
                sim_preds[high_sim_mask],
                sim_labels[high_sim_mask]
            ) * 1.5
        else:
            high_sim_penalty = 0

        # 置信度相關的懲罰
        sim_confidence = confidence_scores[:, 0]
        ent_confidence = confidence_scores[:, 1]

        sim_errors = torch.abs(sim_preds - sim_labels)
        ent_errors = (torch.argmax(ent_preds, dim=1) != ent_labels).float()

        confidence_penalty = self.confidence_penalty * (
            torch.mean(sim_confidence * sim_errors) +
            torch.mean(ent_confidence * ent_errors)
        )

        # NEUTRAL誤判為ENTAILMENT的特別懲罰
        neutral_mask = ent_labels == 0
        if neutral_mask.any():
            neutral_predictions = torch.argmax(ent_preds[neutral_mask], dim=1)
            entail_penalty = torch.sum(
                (neutral_predictions == 1).float()
            ) * 0.5
        else:
            entail_penalty = 0

        # 總損失
        total_loss = (
            sim_loss +
            overestimation_loss +  # 新增的高估懲罰項
            high_sim_penalty +
            ent_loss +
            confidence_penalty +
            entail_penalty
        )

        return total_loss

# ASK WITH Claude
model = MultiLabelModel().to(device)
criterion = PenalizedLoss().to(device)

class_weights = torch.tensor([1.0, 1.2, 1.2]).to(device)  # 增加ENTAILMENT和NEUTRAL的權重

# TODO3: Define your optimizer and loss function
# TODO3-1: Define your Optimizer
optimizer = AdamW(model.parameters(), lr=lr)

# 定義scheduler
scheduler = torch.optim.lr_scheduler.OneCycleLR(
    optimizer,
    max_lr=lr,
    epochs=epochs,
    steps_per_epoch=len(dl_train),
    pct_start=0.1  # warm up期間佔總步數的比例
)


# TODO3-2: Define your loss functions (you should have two)
# Write your code here
similarity_loss_fn = torch.nn.MSELoss()  # 回歸任務用MSE
entailment_loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)  # 分類任務用CrossEntropy # 修改loss function加入權重

# scoring functions
spc = SpearmanCorrCoef().to(device)
acc = Accuracy(task='multiclass', num_classes=3).to(device)  # 因為entailment是3分類任務
f1 = F1Score(task='multiclass', num_classes=3, average='macro').to(device)

# ASK WITH Claude

for ep in range(epochs):
    pbar = tqdm(dl_train)
    pbar.set_description(f"Training epoch [{ep+1}/{epochs}]")
    model.train()
    total_loss = 0

    for batch in pbar:
        # Move data to device
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        relatedness_score = batch['relatedness_score'].float().to(device)
        entailment_labels = batch['entailment_labels'].to(device)

        optimizer.zero_grad()

        # 前向傳播 - 現在包含confidence輸出
        similarity_preds, entailment_logits, confidence = model(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        # 計算帶懲罰的損失
        loss = criterion(
            similarity_preds,
            relatedness_score,
            entailment_logits,
            entailment_labels,
            confidence
        )

        # 反向傳播
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()

        pbar.set_postfix(loss=loss.item())
        total_loss += loss.item()

    # 驗證階段
    pbar = tqdm(dl_validation)
    pbar.set_description(f"Validation epoch [{ep+1}/{epochs}]")
    model.eval()

    similarity_preds_list = []
    similarity_labels_list = []
    entailment_preds_list = []
    entailment_labels_list = []
    confidence_list = []  # 新增：記錄置信度

    with torch.no_grad():
        val_loss = 0
        for batch in pbar:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            similarity_labels = batch['relatedness_score'].float().to(device)
            entailment_labels = batch['entailment_labels'].to(device)

            # 前向傳播
            similarity_preds, entailment_logits, confidence = model(
                input_ids,
                attention_mask
            )

            # 記錄預測結果
            similarity_preds_list.append(similarity_preds)
            similarity_labels_list.append(similarity_labels)

            entailment_preds = torch.argmax(entailment_logits, dim=1)
            entailment_preds_list.append(entailment_preds)
            entailment_labels_list.append(entailment_labels)

            confidence_list.append(confidence)

            # 計算驗證損失
            val_batch_loss = criterion(
                similarity_preds,
                similarity_labels,
                entailment_logits,
                entailment_labels,
                confidence
            )
            val_loss += val_batch_loss.item()

    # 合併所有預測和標籤
    all_similarity_preds = torch.cat(similarity_preds_list)
    all_similarity_labels = torch.cat(similarity_labels_list)
    all_entailment_preds = torch.cat(entailment_preds_list)
    all_entailment_labels = torch.cat(entailment_labels_list)
    all_confidence = torch.cat(confidence_list)

    # 計算評估指標
    spearman = spc(all_similarity_preds, all_similarity_labels)
    accuracy = acc(all_entailment_preds, all_entailment_labels)
    f1_score = f1(all_entailment_preds, all_entailment_labels)

    # 計算額外的分析指標
    sim_confidence = all_confidence[:, 0]
    ent_confidence = all_confidence[:, 1]


    print(f"\nEpoch {ep+1} Summary:")
    print(f"Training Loss: {total_loss/len(dl_train):.4f}")
    print(f"Validation Loss: {val_loss/len(dl_validation):.4f}")
    print(f"\nValidation Metrics:")
    print(f"- Spearman Correlation: {spearman:.4f}")
    print(f"- Accuracy: {accuracy:.4f}")
    print(f"- F1 Score: {f1_score:.4f}")

    # 保存檢查點
    checkpoint = {
        'epoch': ep + 1,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'loss': total_loss/len(dl_train),
        'metrics': {
            'spearman': spearman.item(),
            'accuracy': accuracy.item(),
            'f1': f1_score.item()
        }
    }
    torch.save(checkpoint, f'./ep{ep+1}.ckpt')

"""For test set predictions, you can write perform evaluation simlar to #TODO5."""

# ASK WITH Claude
# Performance comparison experiments

print("\n=== Starting Performance Comparison Experiments ===")

# 1. Single-task Models
class SimilarityOnlyModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.bert = T.BertModel.from_pretrained('bert-base-uncased')
        self.dropout = torch.nn.Dropout(0.1)
        self.similarity_head = torch.nn.Linear(768, 1)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = self.dropout(outputs.pooler_output)
        similarity_scores = self.similarity_head(pooled_output)
        return similarity_scores.squeeze(-1)

class EntailmentOnlyModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.bert = T.BertModel.from_pretrained('bert-base-uncased')
        self.dropout = torch.nn.Dropout(0.1)
        self.entailment_head = torch.nn.Linear(768, 3)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = self.dropout(outputs.pooler_output)
        entailment_logits = self.entailment_head(pooled_output)
        return entailment_logits

def train_single_task(model, task_type, epochs=3):
    model = model.to(device)
    optimizer = AdamW(model.parameters(), lr=lr)

    if task_type == 'similarity':
        criterion = torch.nn.MSELoss()
    else:  # entailment
        criterion = torch.nn.CrossEntropyLoss()

    results = {
        'train_loss': [],
        'val_metrics': []
    }

    for ep in range(epochs):
        # Training
        model.train()
        total_loss = 0
        pbar = tqdm(dl_train)
        pbar.set_description(f"Training {task_type} epoch [{ep+1}/{epochs}]")

        for batch in pbar:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)

            if task_type == 'similarity':
                labels = batch['relatedness_score'].float().to(device)
                outputs = model(input_ids, attention_mask)
            else:
                labels = batch['entailment_labels'].to(device)
                outputs = model(input_ids, attention_mask)

            loss = criterion(outputs, labels)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            pbar.set_postfix(loss=loss.item())

        avg_loss = total_loss / len(dl_train)
        results['train_loss'].append(avg_loss)

        # Validation
        model.eval()
        val_preds = []
        val_labels = []

        with torch.no_grad():
            for batch in tqdm(dl_validation, desc=f"Validation {task_type} epoch [{ep+1}/{epochs}]"):
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)

                if task_type == 'similarity':
                    labels = batch['relatedness_score'].float().to(device)
                    outputs = model(input_ids, attention_mask)
                else:
                    labels = batch['entailment_labels'].to(device)
                    outputs = model(input_ids, attention_mask)
                    outputs = torch.argmax(outputs, dim=1)

                # 保持在GPU上
                val_preds.append(outputs)
                val_labels.append(labels)

        # 在GPU上合併預測結果
        val_preds = torch.cat(val_preds)
        val_labels = torch.cat(val_labels)

        # 計算指標
        if task_type == 'similarity':
            spearman = spc(val_preds, val_labels)
            results['val_metrics'].append({'spearman': spearman.item()})
            print(f"Spearman correlation: {spearman:.4f}")
        else:
            accuracy = acc(val_preds, val_labels)
            f1_score = f1(val_preds, val_labels)
            results['val_metrics'].append({
                'accuracy': accuracy.item(),
                'f1': f1_score.item()
            })
            print(f"Accuracy: {accuracy:.4f}, F1: {f1_score:.4f}")

    return results

def print_comparison():
    print("\n=== Performance Comparison ===")
    print("\nSingle-task Models Results:")
    print("Similarity (Spearman):", [f"{m['spearman']:.4f}" for m in sim_results['val_metrics']])
    print("Entailment Results:", ent_results['val_metrics'])

# Run experiments
print("\nTraining Similarity-Only Model...")
sim_model = SimilarityOnlyModel()
sim_results = train_single_task(sim_model, 'similarity')

print("\nTraining Entailment-Only Model...")
ent_model = EntailmentOnlyModel()
ent_results = train_single_task(ent_model, 'entailment')

# 顯示結果
print_comparison()

# ASK WITH Claude
# error analysis
import pandas as pd

def error_analysis(model, dataloader, tokenizer):
    model.eval()
    errors = {
        'similarity': [],
        'entailment': []
    }

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Error Analysis"):
            # 移動到GPU
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)

            # 獲取預測 (包含置信度)
            sim_scores, ent_logits, confidence_scores = model(input_ids, attention_mask)
            ent_preds = torch.argmax(ent_logits, dim=1)

            # 獲取置信度
            sim_confidence = confidence_scores[:, 0]
            ent_confidence = confidence_scores[:, 1]

            # 獲取真實標籤
            true_sim = batch['relatedness_score'].float().to(device)
            true_ent = batch['entailment_labels'].to(device)

            # 找出錯誤案例
            sim_errors = torch.abs(sim_scores - true_sim) > 1.0
            ent_errors = ent_preds != true_ent

            # 收集錯誤案例的詳細信息
            for idx in range(len(input_ids)):
                premise = tokenizer.decode(input_ids[idx], skip_special_tokens=True)

                if sim_errors[idx]:
                    errors['similarity'].append({
                        'premise': premise,
                        'predicted': sim_scores[idx].item(),
                        'true': true_sim[idx].item(),
                        'error': abs(sim_scores[idx].item() - true_sim[idx].item()),
                        'confidence': sim_confidence[idx].item() # 新增: 記錄相似度置信度
                    })

                if ent_errors[idx]:
                    errors['entailment'].append({
                        'premise': premise,
                        'predicted': ent_preds[idx].item(),
                        'true': true_ent[idx].item(),
                        'confidence': ent_confidence[idx].item() # 新增: 記錄蘊含關係置信度
                    })

    return errors


# 進行錯誤分析
errors = error_analysis(model, dl_validation, tokenizer)

# 分析相關度任務的錯誤
print("\n=== Similarity Task Error Analysis ===")
sim_errors = pd.DataFrame(errors['similarity'])
if len(sim_errors) > 0:
    print(f"\nTotal similarity errors: {len(sim_errors)}")
    print("\nTop 5 largest errors:")
    print(sim_errors.nlargest(5, 'error')[['premise', 'predicted', 'true', 'error']])

    # 錯誤分布分析
    print("\nError distribution:")
    print(sim_errors['error'].describe())

# 分析蘊含任務的錯誤
print("\n=== Entailment Task Error Analysis ===")
ent_errors = pd.DataFrame(errors['entailment'])
if len(ent_errors) > 0:
    print(f"\nTotal entailment errors: {len(ent_errors)}")

    # 混淆矩陣
    confusion = pd.crosstab(
        ent_errors['true'],
        ent_errors['predicted'],
        rownames=['True'],
        colnames=['Predicted']
    )
    print("\nConfusion Matrix:")
    print(confusion)

    # 錯誤案例分析
    print("\nSample error cases:")
    print(ent_errors.sample(5)[['premise', 'predicted', 'true']])

# 錯誤模式分析
def analyze_error_patterns():
    # 1. 句子長度分析
    sim_errors['length'] = sim_errors['premise'].str.len()
    print("\nSimilarity errors by sentence length:")
    print(sim_errors.groupby(pd.qcut(sim_errors['length'], 4))['error'].mean())

    # 2. 語言複雜度分析（使用簡單的指標）
    def complexity_score(text):
        words = text.split()
        return sum(len(word) for word in words) / len(words)

    sim_errors['complexity'] = sim_errors['premise'].apply(complexity_score)
    print("\nErrors by text complexity:")
    print(sim_errors.groupby(pd.qcut(sim_errors['complexity'], 4))['error'].mean())

    # 3. 相關度範圍分析
    print("\nErrors by true similarity score range:")
    print(sim_errors.groupby(pd.qcut(sim_errors['true'], 4))['error'].mean())

    # 新增: 分析置信度與錯誤的關係
    print("\nConfidence analysis for similarity errors:")
    print(sim_errors.groupby(pd.qcut(sim_errors['confidence'], 4))['error'].mean())

    print("\nConfidence analysis for entailment errors:")
    high_conf_errors = (ent_errors['confidence'] > 0.8).sum()
    print(f"Number of high confidence errors (>0.8): {high_conf_errors}")
    print("Average confidence for entailment errors:", ent_errors['confidence'].mean())

analyze_error_patterns()

# 可視化分析
def plot_error_analysis():
    plt.figure(figsize=(15, 5))

    # 1. 相關度錯誤分布
    plt.subplot(1, 3, 1)
    plt.hist(sim_errors['error'], bins=20)
    plt.title('Distribution of Similarity Errors')
    plt.xlabel('Error Magnitude')
    plt.ylabel('Count')

    # 2. 真實值vs預測值散點圖
    plt.subplot(1, 3, 2)
    plt.scatter(sim_errors['true'], sim_errors['predicted'])
    plt.plot([0, 5], [0, 5], 'r--')  # 理想線
    plt.title('True vs Predicted Similarity')
    plt.xlabel('True Similarity')
    plt.ylabel('Predicted Similarity')

    # 3. 錯誤類型的比例
    plt.subplot(1, 3, 3)
    confusion.plot(kind='bar', stacked=True)
    plt.title('Entailment Error Types')
    plt.xlabel('True Label')
    plt.ylabel('Count')

    plt.tight_layout()
    plt.show()

    plt.subplot(2, 2, 4)
    plt.scatter(sim_errors['confidence'], sim_errors['error'])
    plt.title('Confidence vs Error Magnitude')
    plt.xlabel('Confidence')
    plt.ylabel('Error')

    plt.tight_layout()
    plt.show()

plot_error_analysis()