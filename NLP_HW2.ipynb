{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ORAzzQWQ/NLP_2024/blob/main/NLP_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "r9q_Vou7oguQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn\n",
        "import torch.nn.utils.rnn\n",
        "import torch.utils.data\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uX3M7pB8rnTG",
        "outputId": "efb558da-92cd-4e15-836b-a08b37953d92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1cMuL3hF9jefka9RyF4gEBIGGeFGZYHE-\n",
            "From (redirected): https://drive.google.com/uc?id=1cMuL3hF9jefka9RyF4gEBIGGeFGZYHE-&confirm=t&uuid=e3ad78d0-d11a-471d-86fb-817862aa5e88\n",
            "To: /content/arithmetic_NLP.zip\n",
            "100% 27.3M/27.3M [00:00<00:00, 85.2MB/s]\n",
            "Archive:  arithmetic_NLP.zip\n",
            "  inflating: arithmetic_eval.csv     \n",
            "  inflating: arithmetic_train.csv    \n"
          ]
        }
      ],
      "source": [
        "!gdown --id 1cMuL3hF9jefka9RyF4gEBIGGeFGZYHE- -O arithmetic_NLP.zip\n",
        "!unzip arithmetic_NLP.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Cue7df7BssYl",
        "outputId": "02cf2c4a-82cd-4db8-883c-0524ac902d19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0          src   tgt\n",
              "0     2285313  14*(43+20)=   882\n",
              "1      317061     (6+1)*5=    35\n",
              "2      718770    13+32+29=    74\n",
              "3      170195   31*(3-11)=  -248\n",
              "4     2581417     24*49+1=  1177"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-16a80640-a5f1-4a04-bcec-c5140c8863d1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>src</th>\n",
              "      <th>tgt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2285313</td>\n",
              "      <td>14*(43+20)=</td>\n",
              "      <td>882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>317061</td>\n",
              "      <td>(6+1)*5=</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>718770</td>\n",
              "      <td>13+32+29=</td>\n",
              "      <td>74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>170195</td>\n",
              "      <td>31*(3-11)=</td>\n",
              "      <td>-248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2581417</td>\n",
              "      <td>24*49+1=</td>\n",
              "      <td>1177</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-16a80640-a5f1-4a04-bcec-c5140c8863d1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-16a80640-a5f1-4a04-bcec-c5140c8863d1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-16a80640-a5f1-4a04-bcec-c5140c8863d1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f0f41103-826f-486f-b0c5-41f3251fd44f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f0f41103-826f-486f-b0c5-41f3251fd44f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f0f41103-826f-486f-b0c5-41f3251fd44f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_train"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df_train = pd.read_csv('arithmetic_train.csv')\n",
        "df_eval = pd.read_csv('arithmetic_eval.csv')\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0ns46_SqtryG"
      },
      "outputs": [],
      "source": [
        "# Transform the output data to string\n",
        "df_train['tgt'] = df_train['tgt'].apply(lambda x: str(x))\n",
        "df_train['src'] = df_train['src'].add(df_train['tgt'])\n",
        "df_train['len'] = df_train['src'].apply(lambda x: len(x))\n",
        "\n",
        "df_eval['tgt'] = df_eval['tgt'].apply(lambda x: str(x))\n",
        "df_eval['src'] = df_eval['src'].add(df_eval['tgt'])\n",
        "df_eval['len'] = df_eval['src'].apply(lambda x: len(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXnZsbZ1xgmG",
        "outputId": "43fc6644-5d50-46df-c542-d2a929bae7f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size: 18\n"
          ]
        }
      ],
      "source": [
        "char_to_id = {}\n",
        "id_to_char = {}\n",
        "\n",
        "characters = ['<pad>', '<eos>', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '+', '-', '*', '(', ')', '=']\n",
        "for idx, char in enumerate(characters):\n",
        "    char_to_id[char] = idx\n",
        "    id_to_char[idx] = char\n",
        "\n",
        "vocab_size = len(char_to_id)\n",
        "\n",
        "print('vocab_size: {}'.format(vocab_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "dEqUE-s9_dL1",
        "outputId": "adbe8db6-bb1b-412a-d611-e34e616cc8fc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              src   tgt  len  \\\n",
              "0  14*(43+20)=882   882   14   \n",
              "1      (6+1)*5=35    35   10   \n",
              "2     13+32+29=74    74   11   \n",
              "3  31*(3-11)=-248  -248   14   \n",
              "4    24*49+1=1177  1177   12   \n",
              "\n",
              "                                        char_id_list  \\\n",
              "0  [3, 6, 14, 15, 6, 5, 12, 4, 2, 16, 17, 10, 10,...   \n",
              "1             [15, 8, 12, 3, 16, 14, 7, 17, 5, 7, 1]   \n",
              "2           [3, 5, 12, 5, 4, 12, 4, 11, 17, 9, 6, 1]   \n",
              "3  [5, 3, 14, 15, 5, 13, 3, 3, 16, 17, 13, 4, 6, ...   \n",
              "4        [4, 6, 14, 6, 11, 12, 3, 17, 3, 3, 9, 9, 1]   \n",
              "\n",
              "                                     label_id_list  \n",
              "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 10, 4, 1]  \n",
              "1                [0, 0, 0, 0, 0, 0, 0, 0, 5, 7, 1]  \n",
              "2             [0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 6, 1]  \n",
              "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 4, 6, 10, 1]  \n",
              "4          [0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 9, 9, 1]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1b7d94fe-975a-4af2-936b-b8f49e3a8a34\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>src</th>\n",
              "      <th>tgt</th>\n",
              "      <th>len</th>\n",
              "      <th>char_id_list</th>\n",
              "      <th>label_id_list</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14*(43+20)=882</td>\n",
              "      <td>882</td>\n",
              "      <td>14</td>\n",
              "      <td>[3, 6, 14, 15, 6, 5, 12, 4, 2, 16, 17, 10, 10,...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 10, 4, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(6+1)*5=35</td>\n",
              "      <td>35</td>\n",
              "      <td>10</td>\n",
              "      <td>[15, 8, 12, 3, 16, 14, 7, 17, 5, 7, 1]</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 5, 7, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13+32+29=74</td>\n",
              "      <td>74</td>\n",
              "      <td>11</td>\n",
              "      <td>[3, 5, 12, 5, 4, 12, 4, 11, 17, 9, 6, 1]</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 6, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>31*(3-11)=-248</td>\n",
              "      <td>-248</td>\n",
              "      <td>14</td>\n",
              "      <td>[5, 3, 14, 15, 5, 13, 3, 3, 16, 17, 13, 4, 6, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 4, 6, 10, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>24*49+1=1177</td>\n",
              "      <td>1177</td>\n",
              "      <td>12</td>\n",
              "      <td>[4, 6, 14, 6, 11, 12, 3, 17, 3, 3, 9, 9, 1]</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 9, 9, 1]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b7d94fe-975a-4af2-936b-b8f49e3a8a34')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1b7d94fe-975a-4af2-936b-b8f49e3a8a34 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1b7d94fe-975a-4af2-936b-b8f49e3a8a34');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7ac6aeaf-561f-48e6-8cba-5470340423e1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7ac6aeaf-561f-48e6-8cba-5470340423e1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7ac6aeaf-561f-48e6-8cba-5470340423e1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"# df_eval\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"src\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"(6+1)*5=35\",\n          \"24*49+1=1177\",\n          \"13+32+29=74\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tgt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"35\",\n          \"1177\",\n          \"74\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"len\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 10,\n        \"max\": 14,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          10,\n          12,\n          14\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"char_id_list\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label_id_list\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "def char_id(expr, token_map):\n",
        "    tokens = re.findall(r'\\d|[+\\-*/=()]', expr)\n",
        "    id_list = [token_map[token] for token in tokens if token in token_map]\n",
        "    id_list.append(token_map['<eos>'])\n",
        "    return id_list\n",
        "\n",
        "def label_id(char_id_list, token_map):\n",
        "    equal_pos = char_id_list.index(token_map['='])\n",
        "    return [0] * (equal_pos+1) + char_id_list[equal_pos + 1:]\n",
        "\n",
        "df_train['char_id_list'] = df_train['src'].apply(lambda x: char_id(x, char_to_id))\n",
        "df_train['label_id_list'] = df_train['char_id_list'].apply(label_id, token_map=char_to_id)  # 等號後的\n",
        "df_train = df_train[['src', 'tgt', 'len', 'char_id_list', 'label_id_list']]\n",
        "\n",
        "df_eval['char_id_list'] = df_eval['src'].apply(lambda x: char_id(x, char_to_id))\n",
        "df_eval['label_id_list'] = df_eval['char_id_list'].apply(label_id, token_map=char_to_id)\n",
        "df_eval = df_eval[['src', 'tgt', 'len', 'char_id_list', 'label_id_list']]\n",
        "\n",
        "df_train.head()\n",
        "# df_train.to_csv('df_train.csv', index=False)\n",
        "# df_eval.to_csv('df_eval.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "V41suXVlubi9"
      },
      "outputs": [],
      "source": [
        "# Model (ASK Claude and TA example)\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, sequences):\n",
        "        self.sequences = sequences\n",
        "\n",
        "    def __len__(self):\n",
        "        # return how much data is here in the Dataset object\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Extract the input data x and the ground truth y from the data\n",
        "        data = self.sequences.iloc[index]\n",
        "        x = torch.tensor(data['char_id_list'])\n",
        "        y = torch.tensor(data['label_id_list'])\n",
        "        return x, y\n",
        "\n",
        "# Model Implementation\n",
        "class CharRNN(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
        "        super(CharRNN, self).__init__()  # 修正：加上括號\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = torch.nn.Embedding(num_embeddings=vocab_size,\n",
        "                                          embedding_dim=embed_dim,\n",
        "                                          padding_idx=char_to_id['<pad>'])\n",
        "\n",
        "        # Two LSTM layers\n",
        "        self.rnn_layer1 = torch.nn.LSTM(input_size=embed_dim,\n",
        "                                       hidden_size=hidden_dim,\n",
        "                                       batch_first=True)\n",
        "\n",
        "        self.rnn_layer2 = torch.nn.LSTM(input_size=hidden_dim,\n",
        "                                       hidden_size=hidden_dim,\n",
        "                                       batch_first=True)\n",
        "\n",
        "        # Sequential layer with linear transformations and ReLU\n",
        "        self.linear = torch.nn.Sequential(\n",
        "            torch.nn.Linear(in_features=hidden_dim, out_features=hidden_dim),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(in_features=hidden_dim, out_features=vocab_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, target=None):\n",
        "        # x shape: (batch_size, sequence_length)\n",
        "        batch_size = x.size(0)\n",
        "        sequence_length = x.size(1)\n",
        "\n",
        "        # 1. 嵌入層處理所有輸入\n",
        "        embedded = self.embedding(x)\n",
        "        # embedded shape: (batch_size, sequence_length, embed_dim)\n",
        "\n",
        "        # 2. 通過 LSTM 層\n",
        "        output1, _ = self.rnn_layer1(embedded)\n",
        "        output2, _ = self.rnn_layer2(output1)\n",
        "        # output2 shape: (batch_size, sequence_length, hidden_dim)\n",
        "\n",
        "        # 3. 通過線性層得到預測\n",
        "        outputs = self.linear(output2)\n",
        "        # outputs shape: (batch_size, sequence_length, vocab_size)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def generator(self, start_char, max_len=200):\n",
        "        # Convert input characters to IDs\n",
        "        char_list = [char_to_id[c] for c in start_char]\n",
        "\n",
        "        next_char = None\n",
        "\n",
        "        while len(char_list) < max_len:\n",
        "            # Pack the char_list to tensor\n",
        "            x = torch.tensor(char_list).unsqueeze(0).to(next(self.parameters()).device)\n",
        "\n",
        "            # Input the tensor through the model layers\n",
        "            embedded = self.embedding(x)\n",
        "            output1, _ = self.rnn_layer1(embedded)\n",
        "            output2, _ = self.rnn_layer2(output1)\n",
        "            y = self.linear(output2)\n",
        "\n",
        "            # Obtain the next token prediction\n",
        "            y = y[:, -1, :]  # Get the last prediction\n",
        "\n",
        "            # Use argmax function to get the next token prediction\n",
        "            next_char = torch.argmax(y, dim=-1).item()\n",
        "\n",
        "            if next_char == char_to_id['<eos>']:\n",
        "                break\n",
        "\n",
        "            char_list.append(next_char)\n",
        "\n",
        "        # Convert IDs back to characters\n",
        "        return [id_to_char[ch_id] for ch_id in char_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0EhHID_s0Xx3"
      },
      "outputs": [],
      "source": [
        "# (ASK Claude and ChatGPT)\n",
        "def collate_fn(batch):\n",
        "    # 把序列和標籤分開\n",
        "    sequences, labels = zip(*batch)\n",
        "\n",
        "    # 使用 pad_sequence 自動處理填充\n",
        "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=char_to_id['<pad>'])\n",
        "    padded_labels = pad_sequence(labels, batch_first=True, padding_value=char_to_id['<pad>'])\n",
        "\n",
        "    return padded_sequences, padded_labels\n",
        "\n",
        "def train_step(model, optimizer, criterion, x, y):\n",
        "    # 1. 獲取模型預測\n",
        "    logits = model(x)\n",
        "\n",
        "    # 2. 計算損失（注意：y需要錯一位，因為我們在預測下一個字符）\n",
        "    loss = criterion(\n",
        "        logits[:, :-1].reshape(-1, logits.size(-1)),  # 除去最後一個預測\n",
        "        y[:, 1:].reshape(-1)  # 除去第一個目標\n",
        "    )\n",
        "\n",
        "    # 3. 反向傳播和優化\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "# 使用示例\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (ASK Claude and ChatGPT)\n",
        "model = CharRNN(vocab_size=len(char_to_id), embed_dim=64, hidden_dim=128)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=char_to_id['<pad>'])\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
        "\n",
        "num_epochs = 10\n",
        "train_dataset = Dataset(df_train)\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=16,  # 減小 batch_size\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# 訓練循環\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    batch_count = 0\n",
        "\n",
        "    try:\n",
        "        for batch_idx, (x, y) in enumerate(train_loader):\n",
        "            loss = train_step(model, optimizer, criterion, x, y)\n",
        "            total_loss += loss\n",
        "            batch_count += 1\n",
        "\n",
        "            if (batch_idx + 1) % 1000 == 0:\n",
        "                avg_loss = total_loss / batch_count\n",
        "                print(f\"Epoch {epoch+1}, Batch {batch_idx+1}, Loss: {loss:.4f}, Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # 每個 epoch 結束後更新學習率\n",
        "        epoch_loss = total_loss / batch_count\n",
        "        scheduler.step(epoch_loss)\n",
        "        print(f\"Epoch {epoch+1} completed, Average Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in epoch {epoch+1}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "# 保存模型\n",
        "torch.save(model.state_dict(), 'char_rnn_model.pth')\n",
        "\n",
        "# # 加载模型\n",
        "# model.load_state_dict(torch.load('char_rnn_model.pth'))\n",
        "# model.eval()\n"
      ],
      "metadata": {
        "id": "lKuoauQMikZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (ASK Claude and ChatGPT)\n",
        "def evaluate_model(model, eval_loader, device, char_to_id, id_to_char):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    criterion = torch.nn.CrossEntropyLoss(ignore_index=char_to_id['<pad>'])\n",
        "\n",
        "    # 用于存储每个字符的准确率统计\n",
        "    char_stats = {char: {'correct': 0, 'total': 0} for char in char_to_id.keys()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (input_seq, target_seq) in enumerate(eval_loader):\n",
        "            input_seq = input_seq.to(device)\n",
        "            target_seq = target_seq.to(device)\n",
        "\n",
        "            # 获取模型输出\n",
        "            output = model(input_seq)\n",
        "\n",
        "            # 计算损失\n",
        "            loss = criterion(\n",
        "                output[:, :-1].reshape(-1, len(char_to_id)),  # 除去最后一个预测\n",
        "                target_seq[:, 1:].reshape(-1)  # 除去第一个目标\n",
        "            )\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # 获取预测\n",
        "            _, predicted = torch.max(output[:, :-1].reshape(-1, len(char_to_id)), dim=1)\n",
        "            targets = target_seq[:, 1:].reshape(-1)\n",
        "\n",
        "            # 只考虑非填充字符的预测\n",
        "            mask = targets != char_to_id['<pad>']\n",
        "            predicted = predicted[mask]\n",
        "            targets = targets[mask]\n",
        "\n",
        "            # 统计每个字符的准确率\n",
        "            for pred, targ in zip(predicted, targets):\n",
        "                pred_char = id_to_char[pred.item()]\n",
        "                targ_char = id_to_char[targ.item()]\n",
        "\n",
        "                char_stats[targ_char]['total'] += 1\n",
        "                if pred_char == targ_char:\n",
        "                    char_stats[targ_char]['correct'] += 1\n",
        "                    correct += 1\n",
        "                total += 1\n",
        "\n",
        "    accuracy = (correct / total * 100) if total > 0 else 0\n",
        "    avg_loss = total_loss / len(eval_loader)\n",
        "\n",
        "    char_accuracies = {}\n",
        "    for char, stats in char_stats.items():\n",
        "        if stats['total'] > 0:\n",
        "            char_accuracies[char] = (stats['correct'] / stats['total'] * 100)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'loss': avg_loss,\n",
        "        'char_accuracies': char_accuracies\n",
        "    }\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    model = CharRNN(vocab_size=len(char_to_id), embed_dim=64, hidden_dim=128)\n",
        "    model.load_state_dict(torch.load('char_rnn_model_v3', map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    eval_dataset = Dataset(df_eval)\n",
        "    eval_loader = torch.utils.data.DataLoader(\n",
        "        eval_dataset,\n",
        "        batch_size=16,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    results = evaluate_model(model, eval_loader, device, char_to_id, id_to_char)\n",
        "\n",
        "    print(f\"\\nEvaluation Results:\")\n",
        "    print(f\"Overall Accuracy: {results['accuracy']:.2f}%\")\n",
        "    print(f\"Average Loss: {results['loss']:.4f}\")\n",
        "    print(\"\\nPer-character Accuracy:\")\n",
        "\n",
        "    sorted_chars = sorted(\n",
        "        results['char_accuracies'].items(),\n",
        "        key=lambda x: x[1],\n",
        "        reverse=True\n",
        "    )\n",
        "\n",
        "    for char, acc in sorted_chars:\n",
        "        if char not in ['<pad>', '<eos>']:\n",
        "            print(f\"'{char}': {acc:.2f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "6rgXaOD5UwN3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c96c89b8-eee9-4bef-9199-7d7a4aadfaae"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-a9ba32953fad>:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('char_rnn_model_v3', map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Results:\n",
            "Overall Accuracy: 88.40%\n",
            "Average Loss: 0.2694\n",
            "\n",
            "Per-character Accuracy:\n",
            "'-': 99.94%\n",
            "'0': 93.45%\n",
            "'1': 91.28%\n",
            "'2': 86.78%\n",
            "'5': 81.77%\n",
            "'4': 80.64%\n",
            "'6': 78.51%\n",
            "'8': 77.92%\n",
            "'9': 76.96%\n",
            "'3': 76.89%\n",
            "'7': 73.19%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "1.\tWhat impact does using different learning rates have on model training? (ASK Claude)\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "import time\n",
        "\n",
        "def prepare_subset_data(df_train, subset_ratio=0.5):\n",
        "    \"\"\"準備子集數據\"\"\"\n",
        "    # 隨機抽樣50%的數據\n",
        "    subset_size = int(len(df_train) * subset_ratio)\n",
        "    subset_indices = np.random.choice(len(df_train), subset_size, replace=False)\n",
        "    return df_train.iloc[subset_indices].reset_index(drop=True)\n",
        "\n",
        "def train_with_lr(model, train_loader, learning_rate, num_epochs=5, device='cuda'):\n",
        "    \"\"\"使用指定學習率訓練模型\"\"\"\n",
        "    model_copy = deepcopy(model)\n",
        "    model_copy.to(device)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=char_to_id['<pad>'])\n",
        "    optimizer = optim.Adam(model_copy.parameters(), lr=learning_rate)\n",
        "\n",
        "    history = {\n",
        "        'loss': [],\n",
        "        'epoch_times': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model_copy.train()\n",
        "        epoch_loss = 0\n",
        "        batch_count = 0\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        for batch_idx, (x, y) in enumerate(train_loader):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model_copy(x)\n",
        "\n",
        "            loss = criterion(\n",
        "                output[:, :-1].reshape(-1, output.size(-1)),\n",
        "                y[:, 1:].reshape(-1)\n",
        "            )\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            batch_count += 1\n",
        "\n",
        "            # 每50個batch輸出一次損失\n",
        "            if batch_count % 1000 == 0:\n",
        "                avg_loss = epoch_loss / batch_count\n",
        "                history['loss'].append(avg_loss)\n",
        "                print(f\"LR {learning_rate}: Epoch {epoch+1}, Batch {batch_count}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "        history['epoch_times'].append(epoch_time)\n",
        "\n",
        "        print(f\"\\nLR {learning_rate}: Epoch {epoch+1} 完成, \"\n",
        "              f\"平均損失: {epoch_loss/batch_count:.4f}, \"\n",
        "              f\"耗時: {epoch_time:.2f}秒\")\n",
        "\n",
        "    return history\n",
        "\n",
        "def analyze_learning_rates():\n",
        "    \"\"\"分析不同學習率的效果\"\"\"\n",
        "    # 設定要測試的學習率\n",
        "    learning_rates = [0.0001, 0.001, 0.01]\n",
        "    histories = {}\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # 準備50%的訓練數據\n",
        "    subset_df = prepare_subset_data(df_train, subset_ratio=0.5)\n",
        "    print(f\"使用數據集大小: {len(subset_df)} (原始數據集的50%)\")\n",
        "\n",
        "    # 初始化模型和數據加載器\n",
        "    base_model = CharRNN(vocab_size=len(char_to_id), embed_dim=64, hidden_dim=128)\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        Dataset(subset_df),\n",
        "        batch_size=32,  # 稍微增加batch_size以加快訓練\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    # 使用不同學習率訓練\n",
        "    for lr in learning_rates:\n",
        "        print(f\"\\n開始訓練 - 學習率: {lr}\")\n",
        "        histories[lr] = train_with_lr(base_model, train_loader, lr, num_epochs=5, device=device)\n",
        "\n",
        "    # 繪製損失曲線\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for lr, history in histories.items():\n",
        "        plt.plot(history['loss'], label=f'學習率 = {lr}')\n",
        "\n",
        "    plt.xlabel('訓練批次 (x50)')\n",
        "    plt.ylabel('損失')\n",
        "    plt.title('不同學習率的訓練損失曲線比較')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.yscale('log')\n",
        "    plt.show()\n",
        "\n",
        "    # 比較訓練時間\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    avg_epoch_times = [np.mean(history['epoch_times']) for history in histories.values()]\n",
        "    plt.bar([str(lr) for lr in learning_rates], avg_epoch_times)\n",
        "    plt.xlabel('學習率')\n",
        "    plt.ylabel('平均每輪訓練時間（秒）')\n",
        "    plt.title('不同學習率的訓練時間比較')\n",
        "    plt.show()\n",
        "\n",
        "    # 計算和顯示結果\n",
        "    print(\"\\n學習率分析結果:\")\n",
        "    print(\"\\n學習率    最終損失    平均每輪時間    損失波動性\")\n",
        "    print(\"-\" * 55)\n",
        "    for lr, history in histories.items():\n",
        "        final_loss = history['loss'][-1]\n",
        "        avg_time = np.mean(history['epoch_times'])\n",
        "        loss_volatility = np.std(history['loss'])\n",
        "        print(f\"{lr:.4f}    {final_loss:.4f}        {avg_time:.2f}秒        {loss_volatility:.4f}\")\n",
        "\n",
        "    return histories\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = analyze_learning_rates()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55CmG2O6qGcZ",
        "outputId": "b371e701-9237-42d6-bdfa-b7706601d6cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "使用數據集大小: 1184625 (原始數據集的50%)\n",
            "\n",
            "開始訓練 - 學習率: 0.0001\n",
            "LR 0.0001: Epoch 1, Batch 1000, Loss: 1.9630\n",
            "LR 0.0001: Epoch 1, Batch 2000, Loss: 1.8099\n",
            "LR 0.0001: Epoch 1, Batch 3000, Loss: 1.7447\n",
            "LR 0.0001: Epoch 1, Batch 4000, Loss: 1.7018\n",
            "LR 0.0001: Epoch 1, Batch 5000, Loss: 1.6695\n",
            "LR 0.0001: Epoch 1, Batch 6000, Loss: 1.6437\n",
            "LR 0.0001: Epoch 1, Batch 7000, Loss: 1.6223\n",
            "LR 0.0001: Epoch 1, Batch 8000, Loss: 1.6041\n",
            "LR 0.0001: Epoch 1, Batch 9000, Loss: 1.5874\n",
            "LR 0.0001: Epoch 1, Batch 10000, Loss: 1.5720\n",
            "LR 0.0001: Epoch 1, Batch 11000, Loss: 1.5580\n",
            "LR 0.0001: Epoch 1, Batch 12000, Loss: 1.5444\n",
            "LR 0.0001: Epoch 1, Batch 13000, Loss: 1.5311\n",
            "LR 0.0001: Epoch 1, Batch 14000, Loss: 1.5181\n",
            "LR 0.0001: Epoch 1, Batch 15000, Loss: 1.5054\n",
            "LR 0.0001: Epoch 1, Batch 16000, Loss: 1.4926\n",
            "LR 0.0001: Epoch 1, Batch 17000, Loss: 1.4801\n",
            "LR 0.0001: Epoch 1, Batch 18000, Loss: 1.4676\n",
            "LR 0.0001: Epoch 1, Batch 19000, Loss: 1.4554\n",
            "LR 0.0001: Epoch 1, Batch 20000, Loss: 1.4435\n",
            "LR 0.0001: Epoch 1, Batch 21000, Loss: 1.4319\n",
            "LR 0.0001: Epoch 1, Batch 22000, Loss: 1.4206\n",
            "LR 0.0001: Epoch 1, Batch 23000, Loss: 1.4095\n",
            "LR 0.0001: Epoch 1, Batch 24000, Loss: 1.3987\n",
            "LR 0.0001: Epoch 1, Batch 25000, Loss: 1.3880\n",
            "LR 0.0001: Epoch 1, Batch 26000, Loss: 1.3776\n",
            "LR 0.0001: Epoch 1, Batch 27000, Loss: 1.3672\n",
            "LR 0.0001: Epoch 1, Batch 28000, Loss: 1.3572\n",
            "LR 0.0001: Epoch 1, Batch 29000, Loss: 1.3473\n",
            "LR 0.0001: Epoch 1, Batch 30000, Loss: 1.3375\n",
            "LR 0.0001: Epoch 1, Batch 31000, Loss: 1.3279\n",
            "LR 0.0001: Epoch 1, Batch 32000, Loss: 1.3185\n",
            "LR 0.0001: Epoch 1, Batch 33000, Loss: 1.3092\n",
            "LR 0.0001: Epoch 1, Batch 34000, Loss: 1.3001\n",
            "LR 0.0001: Epoch 1, Batch 35000, Loss: 1.2911\n",
            "LR 0.0001: Epoch 1, Batch 36000, Loss: 1.2821\n",
            "LR 0.0001: Epoch 1, Batch 37000, Loss: 1.2733\n",
            "\n",
            "LR 0.0001: Epoch 1 完成, 平均損失: 1.2731, 耗時: 243.16秒\n",
            "LR 0.0001: Epoch 2, Batch 1000, Loss: 0.9462\n",
            "LR 0.0001: Epoch 2, Batch 2000, Loss: 0.9372\n",
            "LR 0.0001: Epoch 2, Batch 3000, Loss: 0.9315\n",
            "LR 0.0001: Epoch 2, Batch 4000, Loss: 0.9254\n",
            "LR 0.0001: Epoch 2, Batch 5000, Loss: 0.9187\n",
            "LR 0.0001: Epoch 2, Batch 6000, Loss: 0.9109\n",
            "LR 0.0001: Epoch 2, Batch 7000, Loss: 0.9025\n",
            "LR 0.0001: Epoch 2, Batch 8000, Loss: 0.8939\n",
            "LR 0.0001: Epoch 2, Batch 9000, Loss: 0.8858\n",
            "LR 0.0001: Epoch 2, Batch 10000, Loss: 0.8779\n",
            "LR 0.0001: Epoch 2, Batch 11000, Loss: 0.8707\n",
            "LR 0.0001: Epoch 2, Batch 12000, Loss: 0.8636\n",
            "LR 0.0001: Epoch 2, Batch 13000, Loss: 0.8571\n",
            "LR 0.0001: Epoch 2, Batch 14000, Loss: 0.8509\n",
            "LR 0.0001: Epoch 2, Batch 15000, Loss: 0.8449\n",
            "LR 0.0001: Epoch 2, Batch 16000, Loss: 0.8393\n",
            "LR 0.0001: Epoch 2, Batch 17000, Loss: 0.8338\n",
            "LR 0.0001: Epoch 2, Batch 18000, Loss: 0.8285\n",
            "LR 0.0001: Epoch 2, Batch 19000, Loss: 0.8236\n",
            "LR 0.0001: Epoch 2, Batch 20000, Loss: 0.8187\n",
            "LR 0.0001: Epoch 2, Batch 21000, Loss: 0.8141\n",
            "LR 0.0001: Epoch 2, Batch 22000, Loss: 0.8094\n",
            "LR 0.0001: Epoch 2, Batch 23000, Loss: 0.8048\n",
            "LR 0.0001: Epoch 2, Batch 24000, Loss: 0.8001\n",
            "LR 0.0001: Epoch 2, Batch 25000, Loss: 0.7956\n",
            "LR 0.0001: Epoch 2, Batch 26000, Loss: 0.7911\n",
            "LR 0.0001: Epoch 2, Batch 27000, Loss: 0.7867\n",
            "LR 0.0001: Epoch 2, Batch 28000, Loss: 0.7824\n",
            "LR 0.0001: Epoch 2, Batch 29000, Loss: 0.7780\n",
            "LR 0.0001: Epoch 2, Batch 30000, Loss: 0.7741\n",
            "LR 0.0001: Epoch 2, Batch 31000, Loss: 0.7701\n",
            "LR 0.0001: Epoch 2, Batch 32000, Loss: 0.7663\n",
            "LR 0.0001: Epoch 2, Batch 33000, Loss: 0.7626\n",
            "LR 0.0001: Epoch 2, Batch 34000, Loss: 0.7589\n",
            "LR 0.0001: Epoch 2, Batch 35000, Loss: 0.7554\n",
            "LR 0.0001: Epoch 2, Batch 36000, Loss: 0.7520\n",
            "LR 0.0001: Epoch 2, Batch 37000, Loss: 0.7485\n",
            "\n",
            "LR 0.0001: Epoch 2 完成, 平均損失: 0.7485, 耗時: 241.19秒\n",
            "LR 0.0001: Epoch 3, Batch 1000, Loss: 0.6194\n",
            "LR 0.0001: Epoch 3, Batch 2000, Loss: 0.6157\n",
            "LR 0.0001: Epoch 3, Batch 3000, Loss: 0.6152\n",
            "LR 0.0001: Epoch 3, Batch 4000, Loss: 0.6143\n",
            "LR 0.0001: Epoch 3, Batch 5000, Loss: 0.6121\n",
            "LR 0.0001: Epoch 3, Batch 6000, Loss: 0.6109\n",
            "LR 0.0001: Epoch 3, Batch 7000, Loss: 0.6093\n",
            "LR 0.0001: Epoch 3, Batch 8000, Loss: 0.6078\n",
            "LR 0.0001: Epoch 3, Batch 9000, Loss: 0.6055\n",
            "LR 0.0001: Epoch 3, Batch 10000, Loss: 0.6043\n",
            "LR 0.0001: Epoch 3, Batch 11000, Loss: 0.6024\n",
            "LR 0.0001: Epoch 3, Batch 12000, Loss: 0.6011\n",
            "LR 0.0001: Epoch 3, Batch 13000, Loss: 0.5992\n",
            "LR 0.0001: Epoch 3, Batch 14000, Loss: 0.5975\n",
            "LR 0.0001: Epoch 3, Batch 15000, Loss: 0.5960\n",
            "LR 0.0001: Epoch 3, Batch 16000, Loss: 0.5947\n",
            "LR 0.0001: Epoch 3, Batch 17000, Loss: 0.5931\n",
            "LR 0.0001: Epoch 3, Batch 18000, Loss: 0.5915\n",
            "LR 0.0001: Epoch 3, Batch 19000, Loss: 0.5902\n",
            "LR 0.0001: Epoch 3, Batch 20000, Loss: 0.5887\n",
            "LR 0.0001: Epoch 3, Batch 21000, Loss: 0.5872\n",
            "LR 0.0001: Epoch 3, Batch 22000, Loss: 0.5856\n",
            "LR 0.0001: Epoch 3, Batch 23000, Loss: 0.5841\n",
            "LR 0.0001: Epoch 3, Batch 24000, Loss: 0.5827\n",
            "LR 0.0001: Epoch 3, Batch 25000, Loss: 0.5812\n",
            "LR 0.0001: Epoch 3, Batch 26000, Loss: 0.5797\n",
            "LR 0.0001: Epoch 3, Batch 27000, Loss: 0.5783\n",
            "LR 0.0001: Epoch 3, Batch 28000, Loss: 0.5769\n",
            "LR 0.0001: Epoch 3, Batch 29000, Loss: 0.5757\n",
            "LR 0.0001: Epoch 3, Batch 30000, Loss: 0.5743\n",
            "LR 0.0001: Epoch 3, Batch 31000, Loss: 0.5729\n",
            "LR 0.0001: Epoch 3, Batch 32000, Loss: 0.5715\n",
            "LR 0.0001: Epoch 3, Batch 33000, Loss: 0.5703\n",
            "LR 0.0001: Epoch 3, Batch 34000, Loss: 0.5690\n",
            "LR 0.0001: Epoch 3, Batch 35000, Loss: 0.5676\n",
            "LR 0.0001: Epoch 3, Batch 36000, Loss: 0.5665\n",
            "LR 0.0001: Epoch 3, Batch 37000, Loss: 0.5651\n",
            "\n",
            "LR 0.0001: Epoch 3 完成, 平均損失: 0.5651, 耗時: 241.25秒\n",
            "LR 0.0001: Epoch 4, Batch 1000, Loss: 0.5107\n",
            "LR 0.0001: Epoch 4, Batch 2000, Loss: 0.5130\n",
            "LR 0.0001: Epoch 4, Batch 3000, Loss: 0.5131\n",
            "LR 0.0001: Epoch 4, Batch 4000, Loss: 0.5117\n",
            "LR 0.0001: Epoch 4, Batch 5000, Loss: 0.5115\n",
            "LR 0.0001: Epoch 4, Batch 6000, Loss: 0.5115\n",
            "LR 0.0001: Epoch 4, Batch 7000, Loss: 0.5100\n",
            "LR 0.0001: Epoch 4, Batch 8000, Loss: 0.5089\n",
            "LR 0.0001: Epoch 4, Batch 9000, Loss: 0.5081\n",
            "LR 0.0001: Epoch 4, Batch 10000, Loss: 0.5074\n",
            "LR 0.0001: Epoch 4, Batch 11000, Loss: 0.5063\n",
            "LR 0.0001: Epoch 4, Batch 12000, Loss: 0.5056\n",
            "LR 0.0001: Epoch 4, Batch 13000, Loss: 0.5051\n",
            "LR 0.0001: Epoch 4, Batch 14000, Loss: 0.5041\n",
            "LR 0.0001: Epoch 4, Batch 15000, Loss: 0.5034\n",
            "LR 0.0001: Epoch 4, Batch 16000, Loss: 0.5028\n",
            "LR 0.0001: Epoch 4, Batch 17000, Loss: 0.5025\n",
            "LR 0.0001: Epoch 4, Batch 18000, Loss: 0.5014\n",
            "LR 0.0001: Epoch 4, Batch 19000, Loss: 0.5009\n",
            "LR 0.0001: Epoch 4, Batch 20000, Loss: 0.5000\n",
            "LR 0.0001: Epoch 4, Batch 21000, Loss: 0.4992\n",
            "LR 0.0001: Epoch 4, Batch 22000, Loss: 0.4984\n",
            "LR 0.0001: Epoch 4, Batch 23000, Loss: 0.4977\n",
            "LR 0.0001: Epoch 4, Batch 24000, Loss: 0.4970\n",
            "LR 0.0001: Epoch 4, Batch 25000, Loss: 0.4963\n",
            "LR 0.0001: Epoch 4, Batch 26000, Loss: 0.4958\n",
            "LR 0.0001: Epoch 4, Batch 27000, Loss: 0.4950\n",
            "LR 0.0001: Epoch 4, Batch 28000, Loss: 0.4943\n",
            "LR 0.0001: Epoch 4, Batch 29000, Loss: 0.4935\n",
            "LR 0.0001: Epoch 4, Batch 30000, Loss: 0.4926\n",
            "LR 0.0001: Epoch 4, Batch 31000, Loss: 0.4919\n",
            "LR 0.0001: Epoch 4, Batch 32000, Loss: 0.4911\n",
            "LR 0.0001: Epoch 4, Batch 33000, Loss: 0.4903\n",
            "LR 0.0001: Epoch 4, Batch 34000, Loss: 0.4896\n",
            "LR 0.0001: Epoch 4, Batch 35000, Loss: 0.4889\n",
            "LR 0.0001: Epoch 4, Batch 36000, Loss: 0.4882\n",
            "LR 0.0001: Epoch 4, Batch 37000, Loss: 0.4874\n",
            "\n",
            "LR 0.0001: Epoch 4 完成, 平均損失: 0.4874, 耗時: 240.63秒\n",
            "LR 0.0001: Epoch 5, Batch 1000, Loss: 0.4592\n",
            "LR 0.0001: Epoch 5, Batch 2000, Loss: 0.4602\n",
            "LR 0.0001: Epoch 5, Batch 3000, Loss: 0.4585\n",
            "LR 0.0001: Epoch 5, Batch 4000, Loss: 0.4574\n",
            "LR 0.0001: Epoch 5, Batch 5000, Loss: 0.4581\n",
            "LR 0.0001: Epoch 5, Batch 6000, Loss: 0.4574\n",
            "LR 0.0001: Epoch 5, Batch 7000, Loss: 0.4574\n",
            "LR 0.0001: Epoch 5, Batch 8000, Loss: 0.4565\n",
            "LR 0.0001: Epoch 5, Batch 9000, Loss: 0.4561\n",
            "LR 0.0001: Epoch 5, Batch 10000, Loss: 0.4553\n",
            "LR 0.0001: Epoch 5, Batch 11000, Loss: 0.4547\n",
            "LR 0.0001: Epoch 5, Batch 12000, Loss: 0.4543\n",
            "LR 0.0001: Epoch 5, Batch 13000, Loss: 0.4537\n",
            "LR 0.0001: Epoch 5, Batch 14000, Loss: 0.4530\n",
            "LR 0.0001: Epoch 5, Batch 15000, Loss: 0.4528\n",
            "LR 0.0001: Epoch 5, Batch 16000, Loss: 0.4521\n",
            "LR 0.0001: Epoch 5, Batch 17000, Loss: 0.4517\n",
            "LR 0.0001: Epoch 5, Batch 18000, Loss: 0.4512\n",
            "LR 0.0001: Epoch 5, Batch 19000, Loss: 0.4503\n",
            "LR 0.0001: Epoch 5, Batch 20000, Loss: 0.4499\n",
            "LR 0.0001: Epoch 5, Batch 21000, Loss: 0.4495\n",
            "LR 0.0001: Epoch 5, Batch 22000, Loss: 0.4490\n",
            "LR 0.0001: Epoch 5, Batch 23000, Loss: 0.4483\n",
            "LR 0.0001: Epoch 5, Batch 24000, Loss: 0.4478\n",
            "LR 0.0001: Epoch 5, Batch 25000, Loss: 0.4478\n",
            "LR 0.0001: Epoch 5, Batch 26000, Loss: 0.4471\n",
            "LR 0.0001: Epoch 5, Batch 27000, Loss: 0.4467\n",
            "LR 0.0001: Epoch 5, Batch 28000, Loss: 0.4460\n",
            "LR 0.0001: Epoch 5, Batch 29000, Loss: 0.4458\n",
            "LR 0.0001: Epoch 5, Batch 30000, Loss: 0.4453\n",
            "LR 0.0001: Epoch 5, Batch 31000, Loss: 0.4448\n",
            "LR 0.0001: Epoch 5, Batch 32000, Loss: 0.4443\n",
            "LR 0.0001: Epoch 5, Batch 33000, Loss: 0.4438\n",
            "LR 0.0001: Epoch 5, Batch 34000, Loss: 0.4435\n",
            "LR 0.0001: Epoch 5, Batch 35000, Loss: 0.4431\n",
            "LR 0.0001: Epoch 5, Batch 36000, Loss: 0.4425\n",
            "LR 0.0001: Epoch 5, Batch 37000, Loss: 0.4420\n",
            "\n",
            "LR 0.0001: Epoch 5 完成, 平均損失: 0.4420, 耗時: 243.62秒\n",
            "\n",
            "開始訓練 - 學習率: 0.001\n",
            "LR 0.001: Epoch 1, Batch 1000, Loss: 1.6442\n",
            "LR 0.001: Epoch 1, Batch 2000, Loss: 1.5016\n",
            "LR 0.001: Epoch 1, Batch 3000, Loss: 1.3984\n",
            "LR 0.001: Epoch 1, Batch 4000, Loss: 1.3202\n",
            "LR 0.001: Epoch 1, Batch 5000, Loss: 1.2572\n",
            "LR 0.001: Epoch 1, Batch 6000, Loss: 1.2021\n",
            "LR 0.001: Epoch 1, Batch 7000, Loss: 1.1535\n",
            "LR 0.001: Epoch 1, Batch 8000, Loss: 1.1100\n",
            "LR 0.001: Epoch 1, Batch 9000, Loss: 1.0700\n",
            "LR 0.001: Epoch 1, Batch 10000, Loss: 1.0357\n",
            "LR 0.001: Epoch 1, Batch 11000, Loss: 1.0050\n",
            "LR 0.001: Epoch 1, Batch 12000, Loss: 0.9759\n",
            "LR 0.001: Epoch 1, Batch 13000, Loss: 0.9502\n",
            "LR 0.001: Epoch 1, Batch 14000, Loss: 0.9271\n",
            "LR 0.001: Epoch 1, Batch 15000, Loss: 0.9066\n",
            "LR 0.001: Epoch 1, Batch 16000, Loss: 0.8879\n",
            "LR 0.001: Epoch 1, Batch 17000, Loss: 0.8705\n",
            "LR 0.001: Epoch 1, Batch 18000, Loss: 0.8546\n",
            "LR 0.001: Epoch 1, Batch 19000, Loss: 0.8400\n",
            "LR 0.001: Epoch 1, Batch 20000, Loss: 0.8263\n",
            "LR 0.001: Epoch 1, Batch 21000, Loss: 0.8137\n",
            "LR 0.001: Epoch 1, Batch 22000, Loss: 0.8018\n",
            "LR 0.001: Epoch 1, Batch 23000, Loss: 0.7907\n",
            "LR 0.001: Epoch 1, Batch 24000, Loss: 0.7803\n",
            "LR 0.001: Epoch 1, Batch 25000, Loss: 0.7703\n",
            "LR 0.001: Epoch 1, Batch 26000, Loss: 0.7610\n",
            "LR 0.001: Epoch 1, Batch 27000, Loss: 0.7522\n",
            "LR 0.001: Epoch 1, Batch 28000, Loss: 0.7438\n",
            "LR 0.001: Epoch 1, Batch 29000, Loss: 0.7358\n",
            "LR 0.001: Epoch 1, Batch 30000, Loss: 0.7283\n",
            "LR 0.001: Epoch 1, Batch 31000, Loss: 0.7211\n",
            "LR 0.001: Epoch 1, Batch 32000, Loss: 0.7141\n",
            "LR 0.001: Epoch 1, Batch 33000, Loss: 0.7074\n",
            "LR 0.001: Epoch 1, Batch 34000, Loss: 0.7011\n",
            "LR 0.001: Epoch 1, Batch 35000, Loss: 0.6950\n",
            "LR 0.001: Epoch 1, Batch 36000, Loss: 0.6891\n",
            "LR 0.001: Epoch 1, Batch 37000, Loss: 0.6833\n",
            "\n",
            "LR 0.001: Epoch 1 完成, 平均損失: 0.6832, 耗時: 243.87秒\n",
            "LR 0.001: Epoch 2, Batch 1000, Loss: 0.4730\n",
            "LR 0.001: Epoch 2, Batch 2000, Loss: 0.4711\n",
            "LR 0.001: Epoch 2, Batch 3000, Loss: 0.4699\n",
            "LR 0.001: Epoch 2, Batch 4000, Loss: 0.4686\n",
            "LR 0.001: Epoch 2, Batch 5000, Loss: 0.4677\n",
            "LR 0.001: Epoch 2, Batch 6000, Loss: 0.4654\n",
            "LR 0.001: Epoch 2, Batch 7000, Loss: 0.4635\n",
            "LR 0.001: Epoch 2, Batch 8000, Loss: 0.4618\n",
            "LR 0.001: Epoch 2, Batch 9000, Loss: 0.4603\n",
            "LR 0.001: Epoch 2, Batch 10000, Loss: 0.4584\n",
            "LR 0.001: Epoch 2, Batch 11000, Loss: 0.4566\n",
            "LR 0.001: Epoch 2, Batch 12000, Loss: 0.4550\n",
            "LR 0.001: Epoch 2, Batch 13000, Loss: 0.4536\n",
            "LR 0.001: Epoch 2, Batch 14000, Loss: 0.4524\n",
            "LR 0.001: Epoch 2, Batch 15000, Loss: 0.4510\n",
            "LR 0.001: Epoch 2, Batch 16000, Loss: 0.4495\n",
            "LR 0.001: Epoch 2, Batch 17000, Loss: 0.4480\n",
            "LR 0.001: Epoch 2, Batch 18000, Loss: 0.4468\n",
            "LR 0.001: Epoch 2, Batch 19000, Loss: 0.4455\n",
            "LR 0.001: Epoch 2, Batch 20000, Loss: 0.4442\n",
            "LR 0.001: Epoch 2, Batch 21000, Loss: 0.4430\n",
            "LR 0.001: Epoch 2, Batch 22000, Loss: 0.4418\n",
            "LR 0.001: Epoch 2, Batch 23000, Loss: 0.4404\n",
            "LR 0.001: Epoch 2, Batch 24000, Loss: 0.4391\n",
            "LR 0.001: Epoch 2, Batch 25000, Loss: 0.4379\n",
            "LR 0.001: Epoch 2, Batch 26000, Loss: 0.4367\n",
            "LR 0.001: Epoch 2, Batch 27000, Loss: 0.4356\n",
            "LR 0.001: Epoch 2, Batch 28000, Loss: 0.4343\n",
            "LR 0.001: Epoch 2, Batch 29000, Loss: 0.4331\n",
            "LR 0.001: Epoch 2, Batch 30000, Loss: 0.4319\n",
            "LR 0.001: Epoch 2, Batch 31000, Loss: 0.4308\n",
            "LR 0.001: Epoch 2, Batch 32000, Loss: 0.4297\n",
            "LR 0.001: Epoch 2, Batch 33000, Loss: 0.4286\n",
            "LR 0.001: Epoch 2, Batch 34000, Loss: 0.4275\n",
            "LR 0.001: Epoch 2, Batch 35000, Loss: 0.4263\n",
            "LR 0.001: Epoch 2, Batch 36000, Loss: 0.4251\n",
            "LR 0.001: Epoch 2, Batch 37000, Loss: 0.4240\n",
            "\n",
            "LR 0.001: Epoch 2 完成, 平均損失: 0.4240, 耗時: 245.39秒\n",
            "LR 0.001: Epoch 3, Batch 1000, Loss: 0.3777\n",
            "LR 0.001: Epoch 3, Batch 2000, Loss: 0.3817\n",
            "LR 0.001: Epoch 3, Batch 3000, Loss: 0.3809\n",
            "LR 0.001: Epoch 3, Batch 4000, Loss: 0.3790\n",
            "LR 0.001: Epoch 3, Batch 5000, Loss: 0.3772\n",
            "LR 0.001: Epoch 3, Batch 6000, Loss: 0.3770\n",
            "LR 0.001: Epoch 3, Batch 7000, Loss: 0.3766\n",
            "LR 0.001: Epoch 3, Batch 8000, Loss: 0.3762\n",
            "LR 0.001: Epoch 3, Batch 9000, Loss: 0.3757\n",
            "LR 0.001: Epoch 3, Batch 10000, Loss: 0.3751\n",
            "LR 0.001: Epoch 3, Batch 11000, Loss: 0.3748\n",
            "LR 0.001: Epoch 3, Batch 12000, Loss: 0.3741\n",
            "LR 0.001: Epoch 3, Batch 13000, Loss: 0.3734\n",
            "LR 0.001: Epoch 3, Batch 14000, Loss: 0.3729\n",
            "LR 0.001: Epoch 3, Batch 15000, Loss: 0.3721\n",
            "LR 0.001: Epoch 3, Batch 16000, Loss: 0.3717\n",
            "LR 0.001: Epoch 3, Batch 17000, Loss: 0.3713\n",
            "LR 0.001: Epoch 3, Batch 18000, Loss: 0.3707\n",
            "LR 0.001: Epoch 3, Batch 19000, Loss: 0.3701\n",
            "LR 0.001: Epoch 3, Batch 20000, Loss: 0.3695\n",
            "LR 0.001: Epoch 3, Batch 21000, Loss: 0.3690\n",
            "LR 0.001: Epoch 3, Batch 22000, Loss: 0.3683\n",
            "LR 0.001: Epoch 3, Batch 23000, Loss: 0.3677\n",
            "LR 0.001: Epoch 3, Batch 24000, Loss: 0.3672\n",
            "LR 0.001: Epoch 3, Batch 25000, Loss: 0.3667\n",
            "LR 0.001: Epoch 3, Batch 26000, Loss: 0.3662\n",
            "LR 0.001: Epoch 3, Batch 27000, Loss: 0.3656\n",
            "LR 0.001: Epoch 3, Batch 28000, Loss: 0.3651\n",
            "LR 0.001: Epoch 3, Batch 29000, Loss: 0.3645\n",
            "LR 0.001: Epoch 3, Batch 30000, Loss: 0.3640\n",
            "LR 0.001: Epoch 3, Batch 31000, Loss: 0.3635\n",
            "LR 0.001: Epoch 3, Batch 32000, Loss: 0.3628\n",
            "LR 0.001: Epoch 3, Batch 33000, Loss: 0.3624\n",
            "LR 0.001: Epoch 3, Batch 34000, Loss: 0.3618\n",
            "LR 0.001: Epoch 3, Batch 35000, Loss: 0.3612\n",
            "LR 0.001: Epoch 3, Batch 36000, Loss: 0.3607\n",
            "LR 0.001: Epoch 3, Batch 37000, Loss: 0.3603\n",
            "\n",
            "LR 0.001: Epoch 3 完成, 平均損失: 0.3603, 耗時: 240.11秒\n",
            "LR 0.001: Epoch 4, Batch 1000, Loss: 0.3426\n",
            "LR 0.001: Epoch 4, Batch 2000, Loss: 0.3395\n",
            "LR 0.001: Epoch 4, Batch 3000, Loss: 0.3408\n",
            "LR 0.001: Epoch 4, Batch 4000, Loss: 0.3387\n",
            "LR 0.001: Epoch 4, Batch 5000, Loss: 0.3379\n",
            "LR 0.001: Epoch 4, Batch 6000, Loss: 0.3372\n",
            "LR 0.001: Epoch 4, Batch 7000, Loss: 0.3363\n",
            "LR 0.001: Epoch 4, Batch 8000, Loss: 0.3359\n",
            "LR 0.001: Epoch 4, Batch 9000, Loss: 0.3357\n",
            "LR 0.001: Epoch 4, Batch 10000, Loss: 0.3352\n",
            "LR 0.001: Epoch 4, Batch 11000, Loss: 0.3349\n",
            "LR 0.001: Epoch 4, Batch 12000, Loss: 0.3345\n",
            "LR 0.001: Epoch 4, Batch 13000, Loss: 0.3343\n",
            "LR 0.001: Epoch 4, Batch 14000, Loss: 0.3340\n",
            "LR 0.001: Epoch 4, Batch 15000, Loss: 0.3338\n",
            "LR 0.001: Epoch 4, Batch 16000, Loss: 0.3333\n",
            "LR 0.001: Epoch 4, Batch 17000, Loss: 0.3330\n",
            "LR 0.001: Epoch 4, Batch 18000, Loss: 0.3326\n",
            "LR 0.001: Epoch 4, Batch 19000, Loss: 0.3324\n",
            "LR 0.001: Epoch 4, Batch 20000, Loss: 0.3321\n",
            "LR 0.001: Epoch 4, Batch 21000, Loss: 0.3318\n",
            "LR 0.001: Epoch 4, Batch 22000, Loss: 0.3315\n",
            "LR 0.001: Epoch 4, Batch 23000, Loss: 0.3311\n",
            "LR 0.001: Epoch 4, Batch 24000, Loss: 0.3307\n",
            "LR 0.001: Epoch 4, Batch 25000, Loss: 0.3304\n",
            "LR 0.001: Epoch 4, Batch 26000, Loss: 0.3300\n",
            "LR 0.001: Epoch 4, Batch 27000, Loss: 0.3296\n",
            "LR 0.001: Epoch 4, Batch 28000, Loss: 0.3293\n",
            "LR 0.001: Epoch 4, Batch 29000, Loss: 0.3290\n",
            "LR 0.001: Epoch 4, Batch 30000, Loss: 0.3284\n",
            "LR 0.001: Epoch 4, Batch 31000, Loss: 0.3280\n",
            "LR 0.001: Epoch 4, Batch 32000, Loss: 0.3276\n",
            "LR 0.001: Epoch 4, Batch 33000, Loss: 0.3273\n",
            "LR 0.001: Epoch 4, Batch 34000, Loss: 0.3269\n",
            "LR 0.001: Epoch 4, Batch 35000, Loss: 0.3265\n",
            "LR 0.001: Epoch 4, Batch 36000, Loss: 0.3261\n",
            "LR 0.001: Epoch 4, Batch 37000, Loss: 0.3256\n",
            "\n",
            "LR 0.001: Epoch 4 完成, 平均損失: 0.3256, 耗時: 242.98秒\n",
            "LR 0.001: Epoch 5, Batch 1000, Loss: 0.3066\n",
            "LR 0.001: Epoch 5, Batch 2000, Loss: 0.3067\n",
            "LR 0.001: Epoch 5, Batch 3000, Loss: 0.3056\n",
            "LR 0.001: Epoch 5, Batch 4000, Loss: 0.3050\n",
            "LR 0.001: Epoch 5, Batch 5000, Loss: 0.3053\n",
            "LR 0.001: Epoch 5, Batch 6000, Loss: 0.3054\n",
            "LR 0.001: Epoch 5, Batch 7000, Loss: 0.3060\n",
            "LR 0.001: Epoch 5, Batch 8000, Loss: 0.3060\n",
            "LR 0.001: Epoch 5, Batch 9000, Loss: 0.3058\n",
            "LR 0.001: Epoch 5, Batch 10000, Loss: 0.3054\n",
            "LR 0.001: Epoch 5, Batch 11000, Loss: 0.3058\n",
            "LR 0.001: Epoch 5, Batch 12000, Loss: 0.3057\n",
            "LR 0.001: Epoch 5, Batch 13000, Loss: 0.3051\n",
            "LR 0.001: Epoch 5, Batch 14000, Loss: 0.3047\n",
            "LR 0.001: Epoch 5, Batch 15000, Loss: 0.3048\n",
            "LR 0.001: Epoch 5, Batch 16000, Loss: 0.3047\n",
            "LR 0.001: Epoch 5, Batch 17000, Loss: 0.3045\n",
            "LR 0.001: Epoch 5, Batch 18000, Loss: 0.3042\n",
            "LR 0.001: Epoch 5, Batch 19000, Loss: 0.3039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "2.If you use RNN or GRU instead of LSTM, what will happen to the quality of your answer generation? (ASK Claude)\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import time\n",
        "from copy import deepcopy\n",
        "\n",
        "class BaseRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, rnn_type='lstm'):\n",
        "        super(BaseRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=char_to_id['<pad>'])\n",
        "\n",
        "        # 根據指定類型選擇RNN層\n",
        "        if rnn_type == 'rnn':\n",
        "            self.rnn1 = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
        "            self.rnn2 = nn.RNN(hidden_dim, hidden_dim, batch_first=True)\n",
        "        elif rnn_type == 'gru':\n",
        "            self.rnn1 = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
        "            self.rnn2 = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n",
        "        else:  # lstm\n",
        "            self.rnn1 = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "            self.rnn2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, vocab_size)\n",
        "        )\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output1, _ = self.rnn1(embedded)\n",
        "        output2, _ = self.rnn2(output1)\n",
        "        return self.linear(output2)\n",
        "\n",
        "def train_and_evaluate(model_type, train_loader, eval_loader, device, epochs=5):\n",
        "    \"\"\"訓練並評估指定類型的模型\"\"\"\n",
        "    model = BaseRNN(\n",
        "        vocab_size=len(char_to_id),\n",
        "        embed_dim=64,\n",
        "        hidden_dim=128,\n",
        "        rnn_type=model_type\n",
        "    ).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=char_to_id['<pad>'])\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'eval_loss': [],\n",
        "        'eval_accuracy': [],\n",
        "        'epoch_times': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # 訓練階段\n",
        "        model.train()\n",
        "        epoch_start = time.time()\n",
        "        train_loss = 0\n",
        "        batch_count = 0\n",
        "\n",
        "        for batch_idx, (x, y) in enumerate(train_loader):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(x)\n",
        "            loss = criterion(\n",
        "                output[:, :-1].reshape(-1, output.size(-1)),\n",
        "                y[:, 1:].reshape(-1)\n",
        "            )\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            batch_count += 1\n",
        "\n",
        "            if batch_count % 50 == 0:\n",
        "                print(f\"{model_type.upper()} - Epoch {epoch+1}, Batch {batch_count}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        avg_train_loss = train_loss / batch_count\n",
        "        epoch_time = time.time() - epoch_start\n",
        "\n",
        "        # 評估階段\n",
        "        model.eval()\n",
        "        eval_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x, y in eval_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                output = model(x)\n",
        "\n",
        "                # 計算損失\n",
        "                loss = criterion(\n",
        "                    output[:, :-1].reshape(-1, output.size(-1)),\n",
        "                    y[:, 1:].reshape(-1)\n",
        "                )\n",
        "                eval_loss += loss.item()\n",
        "\n",
        "                # 計算準確率\n",
        "                _, predicted = torch.max(output[:, :-1].reshape(-1, output.size(-1)), 1)\n",
        "                targets = y[:, 1:].reshape(-1)\n",
        "                mask = targets != char_to_id['<pad>']\n",
        "                correct += (predicted[mask] == targets[mask]).sum().item()\n",
        "                total += mask.sum().item()\n",
        "\n",
        "        avg_eval_loss = eval_loss / len(eval_loader)\n",
        "        accuracy = (correct / total * 100) if total > 0 else 0\n",
        "\n",
        "        # 保存結果\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['eval_loss'].append(avg_eval_loss)\n",
        "        history['eval_accuracy'].append(accuracy)\n",
        "        history['epoch_times'].append(epoch_time)\n",
        "\n",
        "        print(f\"\\n{model_type.upper()} Epoch {epoch+1} 結果:\")\n",
        "        print(f\"訓練損失: {avg_train_loss:.4f}\")\n",
        "        print(f\"評估損失: {avg_eval_loss:.4f}\")\n",
        "        print(f\"準確率: {accuracy:.2f}%\")\n",
        "        print(f\"耗時: {epoch_time:.2f}秒\\n\")\n",
        "\n",
        "    return history\n",
        "\n",
        "def compare_models():\n",
        "    \"\"\"比較不同RNN模型的性能\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # 準備數據\n",
        "    subset_df = df_train.sample(frac=0.5, random_state=42)\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        Dataset(subset_df),\n",
        "        batch_size=32,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    eval_loader = torch.utils.data.DataLoader(\n",
        "        Dataset(df_eval),\n",
        "        batch_size=32,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    # 訓練並評估每種模型\n",
        "    model_types = ['rnn', 'gru', 'lstm']\n",
        "    results = {}\n",
        "\n",
        "    for model_type in model_types:\n",
        "        print(f\"\\n開始訓練 {model_type.upper()} 模型...\")\n",
        "        results[model_type] = train_and_evaluate(\n",
        "            model_type, train_loader, eval_loader, device\n",
        "        )\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = compare_models()"
      ],
      "metadata": {
        "id": "DV8fhvVVwUzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "3.  If construct an evaluation set using three-digit numbers while the training set is constructed from two-digit numbers,\n",
        "    what will happen to the quality of your answer gener we ation? (ASK Claude)\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "\n",
        "def generate_three_digit_eval_data(num_samples=1000):\n",
        "    \"\"\"生成三位數的算術驗證數據\"\"\"\n",
        "    data = []\n",
        "    operations = ['+', '-', '*']\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        # 生成三位數\n",
        "        num1 = random.randint(100, 999)\n",
        "        num2 = random.randint(100, 999)\n",
        "        op = random.choice(operations)\n",
        "\n",
        "        # 計算結果\n",
        "        if op == '+':\n",
        "            result = num1 + num2\n",
        "        elif op == '-':\n",
        "            # 確保結果為正數\n",
        "            if num1 < num2:\n",
        "                num1, num2 = num2, num1\n",
        "            result = num1 - num2\n",
        "        else:  # '*'\n",
        "            result = num1 * num2\n",
        "\n",
        "        expr = f\"{num1}{op}{num2}\"\n",
        "\n",
        "        data.append({\n",
        "            'src': expr,\n",
        "            'tgt': str(result)\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # 添加與原始數據集相同的處理\n",
        "    df['tgt'] = df['tgt'].apply(lambda x: str(x))\n",
        "    df['src'] = df['src'].add(df['tgt'])\n",
        "    df['len'] = df['src'].apply(lambda x: len(x))\n",
        "\n",
        "    # 添加字符ID列表\n",
        "    df['char_id_list'] = df['src'].apply(lambda x: char_id(x, char_to_id))\n",
        "    df['label_id_list'] = df['char_id_list'].apply(label_id, token_map=char_to_id)\n",
        "\n",
        "    return df[['src', 'tgt', 'len', 'char_id_list', 'label_id_list']]\n",
        "\n",
        "def evaluate_model(model, eval_loader, device):\n",
        "    \"\"\"評估模型性能\"\"\"\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    operation_stats = {'+': {'correct': 0, 'total': 0},\n",
        "                      '-': {'correct': 0, 'total': 0},\n",
        "                      '*': {'correct': 0, 'total': 0}}\n",
        "    error_examples = []\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss(ignore_index=char_to_id['<pad>'])\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (input_seq, target_seq) in enumerate(eval_loader):\n",
        "            input_seq = input_seq.to(device)\n",
        "            target_seq = target_seq.to(device)\n",
        "\n",
        "            # 獲取模型輸出\n",
        "            output = model(input_seq)\n",
        "\n",
        "            # 計算損失\n",
        "            loss = criterion(\n",
        "                output[:, :-1].reshape(-1, len(char_to_id)),\n",
        "                target_seq[:, 1:].reshape(-1)\n",
        "            )\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # 對每個樣本進行評估\n",
        "            for i in range(input_seq.size(0)):\n",
        "                # 解碼輸入序列（原始算式）\n",
        "                input_chars = [id_to_char[id.item()] for id in input_seq[i]\n",
        "                             if id.item() != char_to_id['<pad>'] and id.item() != char_to_id['<eos>']]\n",
        "                input_expr = ''.join(input_chars)\n",
        "\n",
        "                # 提取運算符\n",
        "                op = next(c for c in input_expr if c in ['+', '-', '*'])\n",
        "\n",
        "                # 解碼目標序列（正確答案）\n",
        "                target_chars = [id_to_char[id.item()] for id in target_seq[i]\n",
        "                              if id.item() != char_to_id['<pad>'] and id.item() != char_to_id['<eos>']]\n",
        "                target_ans = ''.join(target_chars[target_chars.index('=')+1:])  # 只取等號後的答案部分\n",
        "\n",
        "                # 獲取模型預測\n",
        "                pred_logits = output[i]\n",
        "                pred_ids = torch.argmax(pred_logits, dim=-1)\n",
        "                pred_chars = [id_to_char[id.item()] for id in pred_ids\n",
        "                            if id.item() != char_to_id['<pad>'] and id.item() != char_to_id['<eos>']]\n",
        "                pred_ans = ''.join(pred_chars[pred_chars.index('=')+1:])  # 只取等號後的答案部分\n",
        "\n",
        "                # 更新統計信息\n",
        "                operation_stats[op]['total'] += 1\n",
        "                if pred_ans == target_ans:\n",
        "                    total_correct += 1\n",
        "                    operation_stats[op]['correct'] += 1\n",
        "                else:\n",
        "                    # 記錄錯誤示例\n",
        "                    error_examples.append({\n",
        "                        'expression': input_expr,\n",
        "                        'correct': target_ans,\n",
        "                        'predicted': pred_ans,\n",
        "                        'operation': op\n",
        "                    })\n",
        "\n",
        "                total_samples += 1\n",
        "\n",
        "    # 計算準確率\n",
        "    accuracy = total_correct / total_samples * 100\n",
        "    avg_loss = total_loss / len(eval_loader)\n",
        "\n",
        "    # 計算每種運算的準確率\n",
        "    operation_accuracy = {\n",
        "        op: (stats['correct'] / stats['total'] * 100) if stats['total'] > 0 else 0\n",
        "        for op, stats in operation_stats.items()\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'loss': avg_loss,\n",
        "        'operation_accuracy': operation_accuracy,\n",
        "        'error_examples': error_examples[:10],  # 只返回前10個錯誤示例\n",
        "        'total_samples': total_samples\n",
        "    }\n",
        "\n",
        "def compare_performance():\n",
        "    \"\"\"比較模型在二位數和三位數上的表現\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # 加載模型\n",
        "    model = CharRNN(vocab_size=len(char_to_id), embed_dim=64, hidden_dim=128)\n",
        "    model.load_state_dict(torch.load('char_rnn_model_v3', map_location=device))\n",
        "    model.to(device)\n",
        "\n",
        "    # 準備三位數評估數據\n",
        "    df_eval_3digit = generate_three_digit_eval_data(num_samples=1000)\n",
        "    eval_loader_3digit = torch.utils.data.DataLoader(\n",
        "        Dataset(df_eval_3digit),\n",
        "        batch_size=32,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    # 評估原始二位數和新的三位數數據集\n",
        "    eval_loader_2digit = torch.utils.data.DataLoader(\n",
        "        Dataset(df_eval),\n",
        "        batch_size=32,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    print(\"開始評估...\")\n",
        "    results_2digit = evaluate_model(model, eval_loader_2digit, device)\n",
        "    results_3digit = evaluate_model(model, eval_loader_3digit, device)\n",
        "\n",
        "    # 顯示結果\n",
        "    print(\"\\n=== 評估結果比較 ===\")\n",
        "    print(f\"\\n二位數測試集:\")\n",
        "    print(f\"整體準確率: {results_2digit['accuracy']:.2f}%\")\n",
        "    print(f\"平均損失: {results_2digit['loss']:.4f}\")\n",
        "    print(\"\\n各運算準確率:\")\n",
        "    for op, acc in results_2digit['operation_accuracy'].items():\n",
        "        print(f\"{op}: {acc:.2f}%\")\n",
        "\n",
        "    print(f\"\\n三位數測試集:\")\n",
        "    print(f\"整體準確率: {results_3digit['accuracy']:.2f}%\")\n",
        "    print(f\"平均損失: {results_3digit['loss']:.4f}\")\n",
        "    print(\"\\n各運算準確率:\")\n",
        "    for op, acc in results_3digit['operation_accuracy'].items():\n",
        "        print(f\"{op}: {acc:.2f}%\")\n",
        "\n",
        "    print(\"\\n三位數錯誤示例:\")\n",
        "    for i, error in enumerate(results_3digit['error_examples'], 1):\n",
        "        print(f\"\\n錯誤示例 {i}:\")\n",
        "        print(f\"算式: {error['expression']}\")\n",
        "        print(f\"正確答案: {error['correct']}\")\n",
        "        print(f\"模型預測: {error['predicted']}\")\n",
        "        print(f\"運算類型: {error['operation']}\")\n",
        "\n",
        "    return results_2digit, results_3digit\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results_2digit, results_3digit = compare_performance()"
      ],
      "metadata": {
        "id": "_zZjq2VMxROd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "4.\tIf some numbers never appear in your training data, what will happen to your answer generation? (ASK Claude)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "def generate_data_with_missing_numbers(num_samples=1000, missing_numbers=[7]):\n",
        "    \"\"\"生成不包含特定數字的訓練數據\"\"\"\n",
        "    data = []\n",
        "    operations = ['+', '-', '*']\n",
        "\n",
        "    def number_contains_digit(n, digits):\n",
        "        \"\"\"檢查數字是否包含特定數字\"\"\"\n",
        "        return any(str(d) in str(n) for d in digits)\n",
        "\n",
        "    def generate_valid_number(min_val, max_val, missing_digits):\n",
        "        \"\"\"生成不包含特定數字的隨機數\"\"\"\n",
        "        while True:\n",
        "            num = random.randint(min_val, max_val)\n",
        "            if not number_contains_digit(num, missing_digits):\n",
        "                return num\n",
        "\n",
        "    # 生成訓練數據\n",
        "    for _ in range(num_samples):\n",
        "        num1 = generate_valid_number(10, 99, missing_numbers)\n",
        "        num2 = generate_valid_number(10, 99, missing_numbers)\n",
        "        op = random.choice(operations)\n",
        "\n",
        "        # 計算結果\n",
        "        if op == '+':\n",
        "            result = num1 + num2\n",
        "        elif op == '-':\n",
        "            if num1 < num2:\n",
        "                num1, num2 = num2, num1\n",
        "            result = num1 - num2\n",
        "        else:  # '*'\n",
        "            result = num1 * num2\n",
        "\n",
        "        data.append({\n",
        "            'src': f\"{num1}{op}{num2}\",\n",
        "            'tgt': str(result)\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df['tgt'] = df['tgt'].apply(str)\n",
        "    df['src'] = df['src'].add(df['tgt'])\n",
        "    df['len'] = df['src'].apply(len)\n",
        "\n",
        "    return df\n",
        "\n",
        "def generate_test_data_with_missing_numbers(num_samples=200):\n",
        "    \"\"\"生成包含被排除數字的測試數據\"\"\"\n",
        "    data = []\n",
        "    operations = ['+', '-', '*']\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        # 確保至少一個數字包含7\n",
        "        if random.random() < 0.5:\n",
        "            num1 = random.randint(70, 79)  # 確保包含7\n",
        "            num2 = random.randint(10, 99)\n",
        "        else:\n",
        "            num1 = random.randint(10, 99)\n",
        "            num2 = random.randint(70, 79)  # 確保包含7\n",
        "\n",
        "        op = random.choice(operations)\n",
        "\n",
        "        # 計算結果\n",
        "        if op == '+':\n",
        "            result = num1 + num2\n",
        "        elif op == '-':\n",
        "            if num1 < num2:\n",
        "                num1, num2 = num2, num1\n",
        "            result = num1 - num2\n",
        "        else:  # '*'\n",
        "            result = num1 * num2\n",
        "\n",
        "        data.append({\n",
        "            'src': f\"{num1}{op}{num2}\",\n",
        "            'tgt': str(result)\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df['tgt'] = df['tgt'].apply(str)\n",
        "    df['src'] = df['src'].add(df['tgt'])\n",
        "    df['len'] = df['src'].apply(len)\n",
        "\n",
        "    return df\n",
        "\n",
        "def analyze_errors(true_vals, pred_vals):\n",
        "    \"\"\"分析預測錯誤的模式\"\"\"\n",
        "    error_patterns = []\n",
        "\n",
        "    for true, pred in zip(true_vals, pred_vals):\n",
        "        if true != pred:\n",
        "            error_patterns.append({\n",
        "                'true': true,\n",
        "                'predicted': pred,\n",
        "                'true_contains_7': '7' in true,\n",
        "                'error_type': 'substitution' if len(true) == len(pred) else 'length_mismatch'\n",
        "            })\n",
        "\n",
        "    return error_patterns\n",
        "\n",
        "def evaluate_model_on_missing_numbers(model, test_loader, device):\n",
        "    \"\"\"評估模型在包含缺失數字的數據上的表現\"\"\"\n",
        "    model.eval()\n",
        "    results = {\n",
        "        'correct': 0,\n",
        "        'total': 0,\n",
        "        'errors': [],\n",
        "        'predictions': [],\n",
        "        'targets': []\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (input_seq, target_seq) in enumerate(test_loader):\n",
        "            input_seq = input_seq.to(device)\n",
        "            target_seq = target_seq.to(device)\n",
        "\n",
        "            # 獲取模型輸出\n",
        "            output = model(input_seq)\n",
        "\n",
        "            # 解碼預測和目標\n",
        "            for i in range(input_seq.size(0)):\n",
        "                # 解碼輸入\n",
        "                input_chars = [id_to_char[id.item()] for id in input_seq[i]\n",
        "                             if id.item() != char_to_id['<pad>'] and id.item() != char_to_id['<eos>']]\n",
        "                input_expr = ''.join(input_chars)\n",
        "\n",
        "                # 解碼目標\n",
        "                target_chars = [id_to_char[id.item()] for id in target_seq[i]\n",
        "                              if id.item() != char_to_id['<pad>'] and id.item() != char_to_id['<eos>']]\n",
        "                target_ans = ''.join(target_chars[target_chars.index('=')+1:])\n",
        "\n",
        "                # 解碼預測\n",
        "                pred_logits = output[i]\n",
        "                pred_ids = torch.argmax(pred_logits, dim=-1)\n",
        "                pred_chars = [id_to_char[id.item()] for id in pred_ids\n",
        "                            if id.item() != char_to_id['<pad>'] and id.item() != char_to_id['<eos>']]\n",
        "                pred_ans = ''.join(pred_chars[pred_chars.index('=')+1:])\n",
        "\n",
        "                # 記錄結果\n",
        "                results['total'] += 1\n",
        "                if pred_ans == target_ans:\n",
        "                    results['correct'] += 1\n",
        "                else:\n",
        "                    results['errors'].append({\n",
        "                        'expression': input_expr,\n",
        "                        'true': target_ans,\n",
        "                        'predicted': pred_ans,\n",
        "                        'contains_7': '7' in input_expr\n",
        "                    })\n",
        "\n",
        "                results['predictions'].append(pred_ans)\n",
        "                results['targets'].append(target_ans)\n",
        "\n",
        "    return results\n",
        "\n",
        "def main():\n",
        "    # 設置設備\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # 生成不含7的訓練數據\n",
        "    train_data = generate_data_with_missing_numbers(num_samples=1000, missing_numbers=[7])\n",
        "    test_data = generate_test_data_with_missing_numbers(num_samples=200)\n",
        "\n",
        "    # 準備數據加載器\n",
        "    train_dataset = Dataset(train_data)\n",
        "    test_dataset = Dataset(test_data)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=32,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=32,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    # 加載模型\n",
        "    model = CharRNN(vocab_size=len(char_to_id), embed_dim=64, hidden_dim=128)\n",
        "    model.load_state_dict(torch.load('char_rnn_model_v3', map_location=device))\n",
        "    model.to(device)\n",
        "\n",
        "    # 評估模型\n",
        "    results = evaluate_model_on_missing_numbers(model, test_loader, device)\n",
        "\n",
        "    # 分析結果\n",
        "    accuracy = results['correct'] / results['total'] * 100\n",
        "    error_patterns = analyze_errors(results['targets'], results['predictions'])\n",
        "\n",
        "    print(f\"\\n=== 評估結果 ===\")\n",
        "    print(f\"整體準確率: {accuracy:.2f}%\")\n",
        "    print(f\"總樣本數: {results['total']}\")\n",
        "    print(f\"正確預測數: {results['correct']}\")\n",
        "\n",
        "    print(\"\\n=== 錯誤分析 ===\")\n",
        "    error_types = Counter([e['error_type'] for e in error_patterns])\n",
        "    print(\"\\n錯誤類型分布:\")\n",
        "    for error_type, count in error_types.items():\n",
        "        print(f\"{error_type}: {count}\")\n",
        "\n",
        "    print(\"\\n包含7的錯誤示例:\")\n",
        "    for error in [e for e in results['errors'] if e['contains_7']][:5]:\n",
        "        print(f\"\\n算式: {error['expression']}\")\n",
        "        print(f\"正確答案: {error['true']}\")\n",
        "        print(f\"模型預測: {error['predicted']}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = main()"
      ],
      "metadata": {
        "id": "xf4cxNDgx6OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "5.\tWhy do we need gradient clipping during training? (ASK Claude)\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "import time\n",
        "\n",
        "def train_with_clipping(model, train_loader, clip_value, num_epochs=5):\n",
        "    \"\"\"使用梯度裁剪的訓練函數\"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    model_copy = deepcopy(model)\n",
        "    optimizer = torch.optim.Adam(model_copy.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=char_to_id['<pad>'])\n",
        "\n",
        "    history = {\n",
        "        'loss': [],\n",
        "        'gradients': [],\n",
        "        'clipped_gradients': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model_copy.train()\n",
        "        epoch_loss = 0\n",
        "        batch_count = 0\n",
        "\n",
        "        for batch_idx, (x, y) in enumerate(train_loader):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 前向傳播\n",
        "            output = model_copy(x)\n",
        "            loss = criterion(\n",
        "                output[:, :-1].reshape(-1, output.size(-1)),\n",
        "                y[:, 1:].reshape(-1)\n",
        "            )\n",
        "\n",
        "            # 反向傳播\n",
        "            loss.backward()\n",
        "\n",
        "            # 記錄原始梯度\n",
        "            original_grad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "                model_copy.parameters(), float('inf')\n",
        "            )\n",
        "            history['gradients'].append(original_grad_norm.item())\n",
        "\n",
        "            # 應用梯度裁剪\n",
        "            clipped_grad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "                model_copy.parameters(), clip_value\n",
        "            )\n",
        "            history['clipped_gradients'].append(clipped_grad_norm.item())\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            batch_count += 1\n",
        "\n",
        "            if batch_count % 50 == 0:\n",
        "                avg_loss = epoch_loss / batch_count\n",
        "                history['loss'].append(avg_loss)\n",
        "                print(f\"Epoch {epoch+1}, Batch {batch_count}, \"\n",
        "                      f\"Loss: {avg_loss:.4f}, \"\n",
        "                      f\"Gradient Norm: {original_grad_norm:.4f}, \"\n",
        "                      f\"Clipped Norm: {clipped_grad_norm:.4f}\")\n",
        "\n",
        "    return history\n",
        "\n",
        "def analyze_gradient_clipping():\n",
        "    \"\"\"分析不同梯度裁剪閾值的效果\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # 準備數據\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        Dataset(df_train),\n",
        "        batch_size=32,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    # 測試不同的裁剪閾值\n",
        "    clip_values = [1.0, 5.0, 10.0]\n",
        "    results = {}\n",
        "\n",
        "    base_model = CharRNN(vocab_size=len(char_to_id), embed_dim=64, hidden_dim=128)\n",
        "    base_model.to(device)\n",
        "\n",
        "    for clip_value in clip_values:\n",
        "        print(f\"\\n訓練模型 - 梯度裁剪閾值: {clip_value}\")\n",
        "        results[clip_value] = train_with_clipping(\n",
        "            base_model, train_loader, clip_value\n",
        "        )\n",
        "\n",
        "    # 繪製結果\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # 損失曲線\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for clip_value, history in results.items():\n",
        "        plt.plot(history['loss'], label=f'Clip={clip_value}')\n",
        "    plt.xlabel('Batch (x50)')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss with Different Gradient Clipping')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # 梯度分布\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for clip_value, history in results.items():\n",
        "        plt.hist(history['gradients'],\n",
        "                bins=50,\n",
        "                alpha=0.5,\n",
        "                label=f'Clip={clip_value}',\n",
        "                density=True)\n",
        "    plt.xlabel('Gradient Norm')\n",
        "    plt.ylabel('Density')\n",
        "    plt.title('Gradient Norm Distribution')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = analyze_gradient_clipping()"
      ],
      "metadata": {
        "id": "UClsvsNTyWMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "import time\n",
        "\n",
        "def train_with_clipping(model, train_loader, clip_value, num_epochs=5):\n",
        "    \"\"\"使用梯度裁剪的訓練函數\"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    model_copy = deepcopy(model)\n",
        "    optimizer = torch.optim.Adam(model_copy.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=char_to_id['<pad>'])\n",
        "\n",
        "    history = {\n",
        "        'loss': [],\n",
        "        'gradients': [],\n",
        "        'clipped_gradients': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model_copy.train()\n",
        "        epoch_loss = 0\n",
        "        batch_count = 0\n",
        "\n",
        "        for batch_idx, (x, y) in enumerate(train_loader):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 前向傳播\n",
        "            output = model_copy(x)\n",
        "            loss = criterion(\n",
        "                output[:, :-1].reshape(-1, output.size(-1)),\n",
        "                y[:, 1:].reshape(-1)\n",
        "            )\n",
        "\n",
        "            # 反向傳播\n",
        "            loss.backward()\n",
        "\n",
        "            # 記錄原始梯度\n",
        "            original_grad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "                model_copy.parameters(), float('inf')\n",
        "            )\n",
        "            history['gradients'].append(original_grad_norm.item())\n",
        "\n",
        "            # 應用梯度裁剪\n",
        "            clipped_grad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "                model_copy.parameters(), clip_value\n",
        "            )\n",
        "            history['clipped_gradients'].append(clipped_grad_norm.item())\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            batch_count += 1\n",
        "\n",
        "            if batch_count % 50 == 0:\n",
        "                avg_loss = epoch_loss / batch_count\n",
        "                history['loss'].append(avg_loss)\n",
        "                print(f\"Epoch {epoch+1}, Batch {batch_count}, \"\n",
        "                      f\"Loss: {avg_loss:.4f}, \"\n",
        "                      f\"Gradient Norm: {original_grad_norm:.4f}, \"\n",
        "                      f\"Clipped Norm: {clipped_grad_norm:.4f}\")\n",
        "\n",
        "    return history\n",
        "\n",
        "def analyze_gradient_clipping():\n",
        "    \"\"\"分析不同梯度裁剪閾值的效果\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # 準備數據\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        Dataset(df_train),\n",
        "        batch_size=32,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    # 測試不同的裁剪閾值\n",
        "    clip_values = [1.0, 5.0, 10.0]\n",
        "    results = {}\n",
        "\n",
        "    base_model = CharRNN(vocab_size=len(char_to_id), embed_dim=64, hidden_dim=128)\n",
        "    base_model.to(device)\n",
        "\n",
        "    for clip_value in clip_values:\n",
        "        print(f\"\\n訓練模型 - 梯度裁剪閾值: {clip_value}\")\n",
        "        results[clip_value] = train_with_clipping(\n",
        "            base_model, train_loader, clip_value\n",
        "        )\n",
        "\n",
        "    # 繪製結果\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # 損失曲線\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for clip_value, history in results.items():\n",
        "        plt.plot(history['loss'], label=f'Clip={clip_value}')\n",
        "    plt.xlabel('Batch (x50)')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss with Different Gradient Clipping')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # 梯度分布\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for clip_value, history in results.items():\n",
        "        plt.hist(history['gradients'],\n",
        "                bins=50,\n",
        "                alpha=0.5,\n",
        "                label=f'Clip={clip_value}',\n",
        "                density=True)\n",
        "    plt.xlabel('Gradient Norm')\n",
        "    plt.ylabel('Density')\n",
        "    plt.title('Gradient Norm Distribution')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = analyze_gradient_clipping()"
      ],
      "metadata": {
        "id": "x3DLhWL1z5U8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}